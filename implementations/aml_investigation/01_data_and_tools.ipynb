{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab9f1d4",
   "metadata": {},
   "source": [
    "# Data & Tools Exploration\n",
    "\n",
    "This notebook walks you through the foundational layer of the AML investigation setup:\n",
    "- What the database looks like and how it's built\n",
    "- How to explore transactions and accounts manually\n",
    "- What the `ReadOnlySqlDatabase` tool is and why it exists\n",
    "- How an agent would \"see\" the database through that tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from aieng.agent_evals.aml_investigation.data import download_dataset_file, normalize_transactions_data\n",
    "from aieng.agent_evals.tools.sql_database import ReadOnlySqlDatabase\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Setting the notebook directory to the project's root folder\n",
    "if Path(\"\").absolute().name == \"eval-agents\":\n",
    "    print(f\"Notebook path is already the root path: {Path('').absolute()}\")\n",
    "else:\n",
    "    os.chdir(Path(\"\").absolute().parent.parent)\n",
    "    print(f\"The notebook path has been set to: {Path('').absolute()}\")\n",
    "\n",
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee671f",
   "metadata": {},
   "source": [
    "## 1. Exploring the dataset\n",
    "\n",
    "We will be using the [IBM Transactions for Anti Money Laundering (AML)](https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml) dataset, which is available on Kaggle. It contains synthetic transaction data designed to mimic real-world financial transactions, including both legitimate and potentially fraudulent activities. The dataset includes various features such as transaction amount, type, origin and destination accounts, timestamps, and a label indicating whether the transaction is fraudulent or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172158d",
   "metadata": {},
   "source": [
    "### 1.1 Downloading the dataset\n",
    "\n",
    "There are 6 datasets available, divided into two groups of three sets. The groups are based on the ratio of illicit transactions in the data:\n",
    "- Group **HI** contains relatively higher illicit transaction ratios (i.e. more laundering activity)\n",
    "- Group **LI** contains relatively lower illicit transaction ratios (i.e. less laundering activity)\n",
    "\n",
    "Each group has three sets of data based on the total number of transactions/accounts: \"Small\", \"Medium\", and \"Large\".\n",
    "\n",
    "You can download any of the six datasets using the `download_dataset_file` function. However, **note that the code tries to load all the data into memory at once, so the \"Medium\" and \"Large\" datasets may cause memory issues on some machines**. For this reason, we recommend starting with the \"Small\" dataset.\n",
    "\n",
    "Each dataset has 3 files that you can download separately:\n",
    "- `<HI/LI>_<Small/Medium/Large>_Trans.csv`: contains transaction data, with each row representing a single transaction.\n",
    "- `<HI/LI>_<Small/Medium/Large>_accounts.csv`: contains account data, with each row representing a single account.\n",
    "- `<HI/LI>_<Small/Medium/Large>_Patterns.txt`: contains ground-truth laundering patterns, which are groups of transactions that are known to be part of the same laundering scheme. Each pattern includes a list of transaction IDs that are involved in that pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89368db6",
   "metadata": {},
   "source": [
    "#### 1.1.1. The transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_transactions_csv = download_dataset_file(illicit_ratio=\"HI\", transactions_size=\"Small\", filename=\"Trans.csv\")\n",
    "print(f\"Path to transactions.csv: {path_to_transactions_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ce63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(path_to_transactions_csv)\n",
    "transactions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57334721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there duplicates?\n",
    "print(f\"Number of duplicate transactions: {transactions_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddea19c",
   "metadata": {},
   "source": [
    "Notice that the transactions dataset needs some cleaning. For example:\n",
    "- There are duplicate transactions that should be removed before analysis.\n",
    "- There are two columns that have the same name \"Account\". Pandas automatically renamed the second one to \"Account.1\", but we should rename them to something more descriptive.\n",
    "\n",
    "We use the `normalize_transactions_data` function to perform these cleaning steps and make the transactions data easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d17029",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = normalize_transactions_data(transactions_df)\n",
    "transactions_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b202f",
   "metadata": {},
   "source": [
    "#### 1.1.2. The accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c08be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_accounts_csv = download_dataset_file(illicit_ratio=\"HI\", transactions_size=\"Small\", filename=\"accounts.csv\")\n",
    "print(f\"Path to accounts.csv: {path_to_accounts_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b10fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = pd.read_csv(path_to_accounts_csv)\n",
    "accounts_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddee34b",
   "metadata": {},
   "source": [
    "Similar to the transactions dataset, we can rename the columns in the accounts dataset to make them easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2eb793",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df.rename(\n",
    "    columns={\n",
    "        \"Bank Name\": \"bank_name\",\n",
    "        \"Bank ID\": \"bank_id\",\n",
    "        \"Account Number\": \"account_number\",\n",
    "        \"Entity ID\": \"entity_id\",\n",
    "        \"Entity Name\": \"entity_name\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "accounts_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78dff88",
   "metadata": {},
   "source": [
    "#### 1.1.3. The patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7bdeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_patterns_txt = download_dataset_file(illicit_ratio=\"HI\", transactions_size=\"Small\", filename=\"Patterns.txt\")\n",
    "print(f\"Path to patterns.txt: {path_to_patterns_txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af8a8d",
   "metadata": {},
   "source": [
    "Laundering patterns start with `BEGIN LAUNDERING ATTEMPT` and end with `END LAUNDERING ATTEMPT`. Each pattern includes a list of transactions that are involved in that pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first laundering pattern\n",
    "begin_prefix = \"BEGIN LAUNDERING ATTEMPT - \"\n",
    "end_prefix = \"END LAUNDERING ATTEMPT\"\n",
    "with open(path_to_patterns_txt, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(begin_prefix):\n",
    "            print(f\"\\n{line.strip()}\")\n",
    "        elif line.startswith(end_prefix):\n",
    "            print(f\"{line.strip()}\")\n",
    "            break\n",
    "        else:\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227686a",
   "metadata": {},
   "source": [
    "## 2. Build the Database\n",
    "\n",
    "With the datasets downloaded and cleaned, we can build a SQLite database that contains the transactions and accounts data. This will allow us to query the data using SQL, which is a common way for agents to interact with databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = Path(\"implementations/aml_investigation/data/aml_transactions.db\")\n",
    "DDL_PATH = Path(\"implementations/aml_investigation/data/schema.ddl\")\n",
    "\n",
    "print(f\"Database exists: {DB_PATH.exists()}\")\n",
    "print(f\"DDL file exists: {DDL_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if the database doesn't exist yet.\n",
    "# It will build the SQLite DB. It may take some time to run.\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        conn.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "\n",
    "        if not DDL_PATH.exists():\n",
    "            raise FileNotFoundError(f\"DDL file not found at {DDL_PATH}\")\n",
    "\n",
    "        with open(DDL_PATH, \"r\") as file:\n",
    "            conn.executescript(file.read())\n",
    "        conn.commit()\n",
    "\n",
    "        # Add new columns to the transactions DataFrame: date, day_of_week, time_of_day\n",
    "        transactions_df[\"date\"] = pd.to_datetime(transactions_df[\"timestamp\"]).dt.date\n",
    "        transactions_df[\"day_of_week\"] = pd.to_datetime(transactions_df[\"timestamp\"]).dt.day_name()\n",
    "        transactions_df[\"time_of_day\"] = pd.to_datetime(transactions_df[\"timestamp\"]).dt.time\n",
    "\n",
    "        # Set Transaction ID as index\n",
    "        transactions_df.set_index(\"transaction_id\", drop=True, inplace=True)\n",
    "\n",
    "        # NOTE: We drop the \"is_laundering\" column since that's the label the\n",
    "        # agent is trying to predict, and it wouldn't be present in a real\n",
    "        # investigation scenario.\n",
    "        transactions_df.drop(columns=[\"is_laundering\"], inplace=True)\n",
    "\n",
    "        accounts_df.to_sql(\"accounts\", conn, if_exists=\"append\", index=False)\n",
    "        transactions_df.to_sql(\"transactions\", conn, if_exists=\"append\")\n",
    "else:\n",
    "    print(\"Database already exists — skipping creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7068c",
   "metadata": {},
   "source": [
    "### 2.1. Understand the Schema\n",
    "\n",
    "The database has two core tables and one convenience view:\n",
    "\n",
    "- **`accounts`** — who owns each account (bank, account number, entity name)\n",
    "- **`transactions`** — every transfer between accounts (amount, currency, timestamp, payment format)\n",
    "- **`account_transactions`** (view) — a flattened, account-centric view of transactions. Each transaction appears **twice**: once as an OUT row for the sender, once as an IN row for the receiver. This makes it easy to query all activity for a single account without a `UNION` every time.\n",
    "\n",
    "Let's look at the raw DDL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DDL_PATH.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20770ff",
   "metadata": {},
   "source": [
    "## 3. Manual Exploration with `pandas` + `sqlite3`\n",
    "\n",
    "Let's get familiar with the data before involving any agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045bdfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Quick sanity check: how many rows in each table?\n",
    "for table in [\"accounts\", \"transactions\"]:\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) AS n FROM {table}\", conn).iloc[0][\"n\"]\n",
    "    print(f\"{table:20s}: {count:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542190d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the accounts table\n",
    "pd.read_sql(\"SELECT * FROM accounts LIMIT 10\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the transactions table\n",
    "pd.read_sql(\"SELECT * FROM transactions LIMIT 10\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85026e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the account_transactions view\n",
    "# Notice each transaction appears as both an IN row and an OUT row\n",
    "sample_tx = pd.read_sql(\"SELECT transaction_id FROM transactions LIMIT 1\", conn).iloc[0][\"transaction_id\"]\n",
    "\n",
    "print(f\"Looking up transaction: {sample_tx}\\n\")\n",
    "pd.read_sql(f\"SELECT * FROM account_transactions WHERE transaction_id = '{sample_tx}'\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ed91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26917348",
   "metadata": {},
   "source": [
    "## 4. The `ReadOnlySqlDatabase` Tool\n",
    "\n",
    "So far we've been using `sqlite3` directly — a regular connection that could run `DROP TABLE` or `DELETE` if we wanted. \n",
    "\n",
    "When an LLM agent runs SQL, we can't have it modifying data. The [`ReadOnlySqlDatabase`](https://github.com/VectorInstitute/eval-agents/blob/main/aieng-eval-agents/aieng/agent_evals/tools/sql_database.py) tool solves this with two layers of protection:\n",
    "\n",
    "1. **AST-level enforcement** — It parses the SQL into a syntax tree using [SQLGlot](https://sqlglot.com/) and rejects any query that contains write operations (`INSERT`, `UPDATE`, `DROP`, etc.), even if hidden inside a CTE or subquery.\n",
    "2. **Row limits + timeouts** — It caps results at `max_rows` (default 100) and cancels slow queries, preventing runaway costs.\n",
    "\n",
    "The tool exposes exactly **two methods** that become the agent's \"tools\":\n",
    "- `get_schema_info()` — returns the table/column names\n",
    "- `execute(query)` — runs a SELECT and returns a markdown table string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7880a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = ReadOnlySqlDatabase(\n",
    "    connection_uri=f\"sqlite:///{DB_PATH}\",\n",
    "    agent_name=\"NotebookExplorer\",\n",
    "    max_rows=10,  # keep output short for this notebook\n",
    ")\n",
    "\n",
    "print(\"Tool created successfully!\")\n",
    "print(f\"Max rows: {db.max_rows}\")\n",
    "print(f\"Agent name: {db.agent_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44192f1",
   "metadata": {},
   "source": [
    "### 4.1. Schema Discovery\n",
    "\n",
    "This is the first thing the agent does on every case — ask \"what tables exist and what columns do they have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = db.get_schema_info()\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ae1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also ask for a specific table only\n",
    "print(db.get_schema_info(table_names=[\"transactions\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57446310",
   "metadata": {},
   "source": [
    "### 4.2. Running Queries Through the Tool\n",
    "\n",
    "Notice that the output is a **markdown table string** — not a DataFrame. This is intentional: the agent receives it as plain text in its context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671804f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = db.execute(\"SELECT * FROM accounts LIMIT 5\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c32cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation query. This is the kind of query the agent would run\n",
    "result = db.execute(\"\"\"\n",
    "    SELECT\n",
    "        account,\n",
    "        COUNT(*) AS tx_count,\n",
    "        COUNT(DISTINCT counterparty) AS unique_counterparties,\n",
    "        SUM(CASE WHEN direction='IN' THEN amount ELSE 0 END) AS total_in,\n",
    "        SUM(CASE WHEN direction='OUT' THEN amount ELSE 0 END) AS total_out\n",
    "    FROM account_transactions\n",
    "    GROUP BY account\n",
    "    ORDER BY tx_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243c2b6",
   "metadata": {},
   "source": [
    "### 4.3. Safety Demo — Write Operations Are Blocked\n",
    "\n",
    "Let's verify the protection actually works. The tool should reject any write operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b97afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting a DELETE. This should be blocked\n",
    "result = db.execute(\"DELETE FROM transactions WHERE 1=1\")\n",
    "print(result)  # Expect: \"Query Error: Security Violation...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting a write hidden inside a CTE — also blocked\n",
    "result = db.execute(\"\"\"\n",
    "    WITH cleanup AS (\n",
    "        DELETE FROM transactions WHERE 1=1\n",
    "    )\n",
    "    SELECT * FROM accounts LIMIT 1\n",
    "\"\"\")\n",
    "print(result)  # Expect: \"Query Error: Security Violation...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7565f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row limit enforcement — we set max_rows=10 above, so this won't return all rows\n",
    "result = db.execute(\"SELECT * FROM transactions\")\n",
    "# Check the last line — it should say \"Truncated at 10 rows\"\n",
    "for line in result.split(\"\\n\")[-3:]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae21ed",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you've seen:\n",
    "\n",
    "1. **The dataset** — the IBM Transactions for Anti Money Laundering (AML) dataset, its structure, and how to download and clean it.\n",
    "2. **The database** — two tables (`accounts`, `transactions`) and a convenience view (`account_transactions`) storing synthetic AML transaction data.\n",
    "3. **Manual exploration** — how to use `pandas` + `sqlite3` to query the data as a developer would.\n",
    "4. **The `ReadOnlySqlDatabase` tool** — the safety-hardened wrapper the agent uses, with AST-level write blocking and row limits.\n",
    "\n",
    "**Next:** In Notebook 2, we'll instantiate the AML agent, explore how to give it tasks, then inspect its reasoning and tool call trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
