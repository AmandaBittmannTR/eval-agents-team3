{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Case Files and Running the Agent\n",
    "\n",
    "In Notebook 1 we explored the raw database and the `ReadOnlySqlDatabase` tool.\n",
    "Now we zoom out and ask: what problem is the agent actually solving, and how do we feed it a case?\n",
    "\n",
    "This notebook covers:\n",
    "1. The real-world AML workflow we modelled\n",
    "2. The data structures that represent a case\n",
    "3. Why the evaluation dataset has four case types and what each one tests\n",
    "4. How to generate cases from the raw data\n",
    "5. Running a single case through the agent and inspecting its output\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** Complete Notebook 1 first. The database must exist at `implementations/aml_investigation/data/aml_transactions.db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ec10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from aieng.agent_evals.aml_investigation.data import (\n",
    "    CaseFile,\n",
    "    CaseRecord,\n",
    "    GroundTruth,\n",
    "    LaunderingPattern,\n",
    "    build_cases,\n",
    "    download_dataset_file,\n",
    "    normalize_transactions_data,\n",
    ")\n",
    "from aieng.agent_evals.aml_investigation.task import AmlInvestigationTask\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Setting the notebook directory to the project's root folder\n",
    "if Path(\"\").absolute().name == \"eval-agents\":\n",
    "    print(f\"Notebook path is already the root path: {Path('').absolute()}\")\n",
    "else:\n",
    "    os.chdir(Path(\"\").absolute().parent.parent)\n",
    "    print(f\"The notebook path has been set to: {Path('').absolute()}\")\n",
    "\n",
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 1. Our Model of the Anti-Money Laundering Investigation Workflow\n",
    "\n",
    "In practice, AML investigations at financial institutions are more complex than what we model here. What we model is the core investigative loop: a transaction gets flagged, a case is opened, an analyst investigates, and the analyst produces a written determination.\n",
    "\n",
    "In our model, the workflow has three stages.\n",
    "\n",
    "First, an external alerting system flags a transaction. This could be a rules engine, an ML model, a law enforcement referral, or a routine sampling process. The system assigns a `trigger_label` to the case, which is a short string describing why the case was opened. Crucially, this label is noisy: it may be a strong signal (e.g. `FAN-OUT`, `LAW_ENFORCEMENT_REFERRAL`) or essentially no signal at all (e.g. `QA_SAMPLE`, `RANDOM_REVIEW`).\n",
    "\n",
    "Second, the case is opened with a structured record containing: a unique `case_id`, the flagged `seed_transaction_id`, the `seed_timestamp` (which marks the end of the investigation window), and a `window_start` timestamp (which marks how far back the analyst should look). The analyst is only expected to reason about events within that window.\n",
    "\n",
    "Third, the analyst investigates by querying the transaction database, identifies whether the activity is consistent with a laundering pattern, and produces a written output: a narrative summary, a verdict (`is_laundering`), a pattern classification, and the specific transaction IDs that form the suspicious chain.\n",
    "\n",
    "The agent mirrors this structure exactly. It receives the case record as a JSON object, queries the database, and returns a structured `AnalystOutput`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## 2. The Data Structures\n",
    "\n",
    "The agent's input and output are structured as Pydantic models. This allows us to enforce a schema at the model level, which simplifies prompt engineering and evaluation.\n",
    "\n",
    "**`CaseFile`** is what the agent receives. It contains only what a real analyst would be given at case open time: no ground truth, no answer.\n",
    "\n",
    "```python\n",
    "class CaseFile(BaseModel):\n",
    "    case_id: str               # unique identifier\n",
    "    seed_transaction_id: str   # the flagged transaction\n",
    "    seed_timestamp: str        # end of the investigation window\n",
    "    window_start: str          # start of the investigation window\n",
    "    trigger_label: str         # why the case was opened (may be noisy)\n",
    "```\n",
    "\n",
    "**`GroundTruth`** records what actually happened. It is never shown to the agent. It is used only by the graders during evaluation.\n",
    "\n",
    "```python\n",
    "class GroundTruth(BaseModel):\n",
    "    is_laundering: bool\n",
    "    pattern_type: LaunderingPattern    # FAN-IN, FAN-OUT, CYCLE, ..., NONE\n",
    "    pattern_description: str\n",
    "    attempt_transaction_ids: str       # comma-separated laundering chain\n",
    "```\n",
    "\n",
    "**`AnalystOutput`** is what the agent must produce. Its schema is enforced at the model level via `output_schema`.\n",
    "\n",
    "```python\n",
    "class AnalystOutput(BaseModel):\n",
    "    summary_narrative: str             # the agent's reasoning\n",
    "    is_laundering: bool\n",
    "    pattern_type: LaunderingPattern\n",
    "    pattern_description: str\n",
    "    flagged_transaction_ids: str       # the agent's identified laundering chain\n",
    "```\n",
    "\n",
    "A **`CaseRecord`** bundles `input: CaseFile` and `expected_output: GroundTruth` together. This is the unit that goes into the Langfuse dataset. The `input` field is sent to the agent; the `expected_output` field is passed to the graders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the structure manually\n",
    "example_case = CaseRecord(\n",
    "    input=CaseFile(\n",
    "        case_id=\"demo-001\",\n",
    "        seed_transaction_id=\"txn-abc\",\n",
    "        seed_timestamp=\"2022-09-15T14:30:00\",\n",
    "        window_start=\"2022-09-01T00:00:00\",\n",
    "        trigger_label=\"QA_SAMPLE\",  # low-signal: gives no hint about laundering\n",
    "    ),\n",
    "    expected_output=GroundTruth(\n",
    "        is_laundering=True,\n",
    "        pattern_type=LaunderingPattern.FAN_OUT,\n",
    "        pattern_description=\"One source dispersing funds to many destinations.\",\n",
    "        attempt_transaction_ids=\"txn-abc,txn-def,txn-ghi\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"--- Input (what the agent sees) ---\")\n",
    "print(example_case.input.model_dump_json(indent=2))\n",
    "\n",
    "print(\"\\n--- Expected Output (hidden from the agent; used for grading) ---\")\n",
    "print(example_case.expected_output.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "## 3. The Four Case Types\n",
    "\n",
    "A robust evaluation dataset needs to test more than just \"can the agent find laundering?\". We deliberately construct four case types, each probing a different failure mode.\n",
    "\n",
    "| Case type | `is_laundering` (ground truth) | `trigger_label` | What it tests |\n",
    "|---|---|---|---|\n",
    "| **True Positive** | `True` | Pattern name (e.g. `FAN-OUT`) | Can the agent correctly identify and describe a real laundering pattern? |\n",
    "| **True Negative** | `False` | Low-signal (`QA_SAMPLE`, `RANDOM_REVIEW`, ...) | Can the agent correctly clear a benign case without over-investigating? |\n",
    "| **False Positive** | `False` | High-signal (`ANOMALOUS_BEHAVIOR_ALERT`, `LAW_ENFORCEMENT_REFERRAL`, ...) | Can the agent resist a misleading trigger and avoid a false alarm? |\n",
    "| **False Negative** | `True` | Low-signal (`QA_SAMPLE`, `RANDOM_REVIEW`, ...) | Can the agent find laundering even when the trigger provides no hint? |\n",
    "\n",
    "The false positive and false negative cases are the most diagnostic. They test whether the agent can reason independently rather than follow the trigger label.\n",
    "\n",
    "### How each type is built\n",
    "\n",
    "**True Positives** are parsed from the `Patterns.txt` file in the Kaggle dataset. This file records every known laundering attempt: the accounts involved, the exact transactions, and the pattern type. The `trigger_label` is set to the pattern name, simulating an alerting system that correctly identified the behaviour.\n",
    "\n",
    "**True Negatives** sample random benign transactions from the dataset. The `trigger_label` is set to one of `QA_SAMPLE`, `RANDOM_REVIEW`, `RETROSPECTIVE_REVIEW`, or `MODEL_MONITORING_SAMPLE`, realistic labels for a routine compliance review that carries no signal about laundering.\n",
    "\n",
    "**False Positives** are built from benign accounts with an unusually high transaction volume on a single day. High volume is a common heuristic alert trigger, so these cases look suspicious at first glance. The trigger label is a high-signal label like `ANOMALOUS_BEHAVIOR_ALERT`, but the ground truth is `is_laundering=False`.\n",
    "\n",
    "**False Negatives** are taken from additional laundering attempts beyond those used as True Positives. The key difference: the `trigger_label` is swapped to a low-signal review label, removing any hint. The agent must discover the laundering through its own investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 4. Generating Case Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CASES_PATH = Path(\"implementations/aml_investigation/data/aml_cases.jsonl\")\n",
    "\n",
    "ILLICIT_RATIO = \"HI\"  # \"HI\" or \"LI\"\n",
    "TRANSACTIONS_SIZE = \"Small\"  # \"Small\", \"Medium\", or \"Large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if cases do not exist yet.\n",
    "# It downloads the dataset from Kaggle and may take a minute if files aren't cached\n",
    "# locally.\n",
    "\n",
    "if not CASES_PATH.exists():\n",
    "    print(\"Downloading dataset files...\")\n",
    "    path_to_transactions_csv = download_dataset_file(ILLICIT_RATIO, TRANSACTIONS_SIZE, \"Trans.csv\")\n",
    "    path_to_patterns_txt = download_dataset_file(ILLICIT_RATIO, TRANSACTIONS_SIZE, \"Patterns.txt\")\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "    print(\"Normalizing transactions...\")\n",
    "    transactions_df = pd.read_csv(path_to_transactions_csv)\n",
    "    transactions_df = normalize_transactions_data(transactions_df)\n",
    "    print(f\"Loaded {len(transactions_df):,} transactions.\")\n",
    "\n",
    "    print(\"Building cases...\")\n",
    "    cases = build_cases(\n",
    "        path_to_patterns_txt,\n",
    "        transactions_df,\n",
    "        num_laundering_cases=5,\n",
    "        num_normal_cases=5,\n",
    "        num_false_negative_cases=3,\n",
    "        num_false_positive_cases=3,\n",
    "        lookback_days=10,  # how far back from the seed transaction the agent should investigate\n",
    "    )\n",
    "    print(f\"Built {len(cases)} cases.\")\n",
    "\n",
    "    CASES_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with CASES_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for record in cases:\n",
    "            f.write(record.model_dump_json() + \"\\n\")\n",
    "    print(f\"Wrote cases to {CASES_PATH}\")\n",
    "else:\n",
    "    print(f\"Cases already exist at {CASES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cases = [json.loads(line) for line in CASES_PATH.read_text().splitlines() if line.strip()]\n",
    "cases = [CaseRecord.model_validate(raw_case) for raw_case in raw_cases]\n",
    "\n",
    "print(f\"Total cases loaded: {len(cases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of all cases\n",
    "summary = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"case_id\": case.input.case_id[:12] + \"...\",\n",
    "            \"trigger_label\": case.input.trigger_label,\n",
    "            \"is_laundering\": case.expected_output.is_laundering,\n",
    "            \"pattern_type\": case.expected_output.pattern_type.value,\n",
    "            \"window_days\": (pd.Timestamp(case.input.seed_timestamp) - pd.Timestamp(case.input.window_start)).days,\n",
    "        }\n",
    "        for case in cases\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify each case into one of the four types\n",
    "_LOW_SIGNAL = {\"QA_SAMPLE\", \"RANDOM_REVIEW\", \"RETROSPECTIVE_REVIEW\", \"MODEL_MONITORING_SAMPLE\"}\n",
    "_HIGH_SIGNAL = {\"ANOMALOUS_BEHAVIOR_ALERT\", \"LAW_ENFORCEMENT_REFERRAL\", \"EXTERNAL_TIP\"}\n",
    "_PATTERN_LABELS = {p.value for p in LaunderingPattern if p != LaunderingPattern.NONE}\n",
    "\n",
    "\n",
    "def classify_case(case: CaseRecord) -> str:\n",
    "    \"\"\"Classify a case record.\"\"\"\n",
    "    label = case.input.trigger_label\n",
    "    is_laundering = case.expected_output.is_laundering\n",
    "    if label in _PATTERN_LABELS and is_laundering:\n",
    "        return \"True Positive\"\n",
    "    if label in _LOW_SIGNAL and not is_laundering:\n",
    "        return \"True Negative\"\n",
    "    if (label in _HIGH_SIGNAL or label in _PATTERN_LABELS) and not is_laundering:\n",
    "        return \"False Positive\"\n",
    "    if label in _LOW_SIGNAL and is_laundering:\n",
    "        return \"False Negative\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "summary[\"case_type\"] = [classify_case(case) for case in cases]\n",
    "print(summary[\"case_type\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print one representative example of each case type\n",
    "for case_type in [\"True Positive\", \"True Negative\", \"False Positive\", \"False Negative\"]:\n",
    "    idx = summary[summary[\"case_type\"] == case_type].index\n",
    "    if len(idx) == 0:\n",
    "        print(f\"[{case_type}] no examples in this dataset\\n\")\n",
    "        continue\n",
    "    case = cases[idx[0]]\n",
    "    print(f\"=== {case_type} ===\")\n",
    "    print(f\"  trigger_label : {case.input.trigger_label}\")\n",
    "    print(f\"  is_laundering : {case.expected_output.is_laundering}\")\n",
    "    print(f\"  pattern_type  : {case.expected_output.pattern_type.value}\")\n",
    "    print(f\"  window        : {case.input.window_start}  to  {case.input.seed_timestamp}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "source": [
    "## 5. The Agent\n",
    "\n",
    "The agent is a Google ADK `LlmAgent` configured with three things:\n",
    "\n",
    "- A detailed system prompt (`ANALYST_PROMPT`) describing the investigation workflow, a strategy for querying the database efficiently (start with aggregates, expand selectively), and the laundering typologies to look for.\n",
    "- Two tools: `get_schema_info()` and `execute(query)` from `ReadOnlySqlDatabase`, the same ones explored in Notebook 1.\n",
    "- A structured output schema that enforces `AnalystOutput`, so the final response is always a valid, parseable object.\n",
    "\n",
    "`AmlInvestigationTask` is a thin wrapper around the agent that:\n",
    "1. Serializes the `CaseFile` to JSON and sends it as the user message\n",
    "2. Streams the agent's response via the ADK runner\n",
    "3. Extracts the final response and parses it into an `AnalystOutput` object\n",
    "\n",
    "It implements the `TaskFunction` protocol expected by the Langfuse experiment harness, so it can be passed directly to `run_experiment`. We will use it that way in Notebook 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "source": [
    "## 6. Running a Single Case\n",
    "\n",
    "Let's run one case manually and watch the agent work.\n",
    "\n",
    "> **Note:** This requires a `.env` file with valid `GOOGLE_API_KEY`, `LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, and `LANGFUSE_HOST` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = AmlInvestigationTask()\n",
    "print(f\"Agent : {task._agent.name}\")\n",
    "print(f\"Model : {task._agent.model}\")\n",
    "print(f\"Tools : {[tool.name for tool in task._agent.tools]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a case type to run.\n",
    "# Try all four types to see how the agent behaves on each.\n",
    "CASE_TYPE_TO_RUN = \"True Positive\"  # options: \"True Positive\", \"True Negative\", \"False Positive\", \"False Negative\"\n",
    "\n",
    "idx = summary[summary[\"case_type\"] == CASE_TYPE_TO_RUN].index\n",
    "if len(idx) == 0:\n",
    "    raise ValueError(f\"No cases of type '{CASE_TYPE_TO_RUN}' found.\")\n",
    "\n",
    "selected_case = cases[idx[0]]\n",
    "print(f\"Running case : {selected_case.input.case_id}\")\n",
    "print(f\"  Type         : {CASE_TYPE_TO_RUN}\")\n",
    "print(f\"  Trigger      : {selected_case.input.trigger_label}\")\n",
    "print(f\"  Window       : {selected_case.input.window_start} to {selected_case.input.seed_timestamp}\")\n",
    "print()\n",
    "print(\"--- Input sent to the agent ---\")\n",
    "print(selected_case.input.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent. This makes live LLM calls and may take 2-3 minutes.\n",
    "agent_output = await task(item={\"input\": selected_case.input.model_dump()})\n",
    "\n",
    "if agent_output is None:\n",
    "    print(\"Agent returned no output. Check your credentials and that the database exists.\")\n",
    "else:\n",
    "    print(\"\\n--- Agent Output ---\")\n",
    "    print(agent_output)\n",
    "\n",
    "    print(\"Agent finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {},
   "source": [
    "## 7. Comparing Agent Output to Ground Truth\n",
    "\n",
    "Before introducing automated graders, let's compare the output by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed186c9a28b402fb0bc4494df01f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if agent_output is not None:\n",
    "    ground_truth = selected_case.expected_output\n",
    "\n",
    "    is_laundering_match = ground_truth.is_laundering == agent_output[\"is_laundering\"]\n",
    "    pattern_match = ground_truth.pattern_type.value == agent_output[\"pattern_type\"]\n",
    "\n",
    "    ground_truth_transaction_ids = {i.strip() for i in ground_truth.attempt_transaction_ids.split(\",\") if i.strip()}\n",
    "    agent_flagged_ids = {i.strip() for i in agent_output[\"flagged_transaction_ids\"].split(\",\") if i.strip()}\n",
    "\n",
    "    print(f\"{'Field':<30} {'Ground Truth':<25} {'Agent Output':<25} {'Match?'}\")\n",
    "    print(\"-\" * 90)\n",
    "    print(\n",
    "        f\"{'is_laundering':<30} {str(ground_truth.is_laundering):<25} {str(agent_output['is_laundering']):<25} {'OK' if is_laundering_match else 'WRONG'}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{'pattern_type':<30} {ground_truth.pattern_type.value:<25} {agent_output['pattern_type'].value:<25} {'OK' if pattern_match else 'WRONG'}\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    print(f\"Ground truth tx IDs  : {ground_truth.attempt_transaction_ids or '(none)'}\")\n",
    "    print(f\"Agent flagged tx IDs : {agent_output['flagged_transaction_ids'] or '(none)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e1581032b452c9409d6c6813c49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if agent_output is not None:\n",
    "    print(\"=== Agent Summary Narrative ===\")\n",
    "    print(agent_output[\"summary_narrative\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cbbc1e968416e875cc15c1202d7eb",
   "metadata": {},
   "source": [
    "## 8. Try the Other Case Types\n",
    "\n",
    "Go back to the cell in section 6 that sets `CASE_TYPE_TO_RUN` and change it to each of the four types. A few things to pay attention to:\n",
    "\n",
    "- **False Positive**: The `trigger_label` suggests something suspicious. Does the agent correctly clear the case, or does it follow the trigger?\n",
    "- **False Negative**: The `trigger_label` is noise. Does the agent still find the laundering pattern through its own investigation?\n",
    "- **True Negative**: A completely benign case. Does the agent close it cleanly without over-reaching?\n",
    "\n",
    "In the **next** notebook, we will introduce automated graders that quantify the agent's performance across these dimensions at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c27b1587741f2af2001be3712ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "await task.close()\n",
    "print(\"Task closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
