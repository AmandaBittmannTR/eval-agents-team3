{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Evaluation Pipeline\n",
                "\n",
                "In Notebook 2 we built case files and ran one case through the agent by hand. This notebook automates that process across the entire dataset and attaches structured graders at three levels: per-item, per-trace, and per-run.\n",
                "\n",
                "This notebook covers:\n",
                "1. Uploading the case dataset to Langfuse\n",
                "2. What each grader measures and why\n",
                "3. Running the full experiment\n",
                "4. Inspecting results in Python and in the Langfuse UI\n",
                "\n",
                "---\n",
                "\n",
                "**Prerequisites:** Complete Notebooks 1 and 2. The case file must exist at `implementations/aml_investigation/data/aml_cases.jsonl`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "from aieng.agent_evals.aml_investigation.graders.item import item_level_deterministic_grader\n",
                "from aieng.agent_evals.aml_investigation.graders.run import run_level_grader\n",
                "from aieng.agent_evals.aml_investigation.graders.trace import trace_deterministic_grader\n",
                "from aieng.agent_evals.aml_investigation.task import AmlInvestigationTask\n",
                "from aieng.agent_evals.evaluation import run_experiment_with_trace_evals\n",
                "from aieng.agent_evals.langfuse import upload_dataset_to_langfuse\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "\n",
                "# Setting the notebook directory to the project's root folder\n",
                "if Path(\"\").absolute().name == \"eval-agents\":\n",
                "    print(f\"Notebook path is already the root path: {Path('').absolute()}\")\n",
                "else:\n",
                "    os.chdir(Path(\"\").absolute().parent.parent)\n",
                "    print(f\"The notebook path has been set to: {Path('').absolute()}\")\n",
                "\n",
                "CASES_PATH = Path(\"implementations/aml_investigation/data/aml_cases.jsonl\")\n",
                "DATASET_NAME = \"aml-investigation-eval\"\n",
                "\n",
                "assert CASES_PATH.exists(), f\"Cases file not found at {CASES_PATH}. Run Notebook 2 first.\"\n",
                "print(f\"Found {sum(1 for line in CASES_PATH.read_text().splitlines() if line.strip())} cases.\")\n",
                "\n",
                "load_dotenv(verbose=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Uploading the Dataset to Langfuse\n",
                "\n",
                "Langfuse acts as the backbone of our evaluation pipeline. Each case file becomes a dataset item in Langfuse: the `input` field (the `CaseFile`) is what gets sent to the agent, and the `expected_output` field (the `GroundTruth`) is stored separately and made available to the graders.\n",
                "\n",
                "`upload_dataset_to_langfuse` reads the JSONL file, creates the dataset if it does not already exist, and upserts items using a deterministic content-based ID. Running this cell twice is safe: existing items are updated in place rather than duplicated."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "await upload_dataset_to_langfuse(dataset_path=str(CASES_PATH), dataset_name=DATASET_NAME)\n",
                "\n",
                "print(f\"Dataset '{DATASET_NAME}' is ready in Langfuse.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Graders\n",
                "\n",
                "We use three layers of graders. Each layer answers a different question about the agent.\n",
                "\n",
                "### 2.1 Item-level graders\n",
                "\n",
                "Item-level graders run once per case and compare the agent's `AnalystOutput` to the `GroundTruth`. They are deterministic: no LLM judge is involved.\n",
                "\n",
                "| Metric | What it measures |\n",
                "|---|---|\n",
                "| `is_laundering_correct` | Whether the agent's verdict (laundering or not) matches ground truth |\n",
                "| `pattern_type_correct` | Whether the agent named the exact correct pattern (e.g. `FAN-OUT`, `NONE`) |\n",
                "| `non_laundering_pattern_consistent` | When the agent predicts benign, whether the pattern is `NONE` |\n",
                "| `non_laundering_flags_empty` | When the agent predicts benign, whether no transaction IDs are flagged |\n",
                "| `id_precision_like` | Of the flagged IDs, how many were correct minus how many were wrong, normalized by count |\n",
                "| `id_coverage` | Of the ground truth laundering IDs, what fraction the agent correctly identified |\n",
                "\n",
                "The two consistency checks (`non_laundering_pattern_consistent` and `non_laundering_flags_empty`) probe a specific failure mode: an agent that correctly says \"not laundering\" but still outputs a suspicious-looking pattern name or a non-empty list of flagged IDs. These outputs would confuse a downstream consumer even if the verdict is right.\n",
                "\n",
                "### 2.2 Trace-level graders\n",
                "\n",
                "Trace-level graders inspect the agent's SQL tool calls from the Langfuse trace, not just the final output. They run in a second pass after the experiment finishes, once all traces have been ingested.\n",
                "\n",
                "| Metric | What it measures |\n",
                "|---|---|\n",
                "| `trace_has_sql_queries` | Whether the agent issued at least one database query |\n",
                "| `trace_read_only_query_check` | Whether all queries were read-only (no `INSERT`, `UPDATE`, `DROP`, etc.) |\n",
                "| `trace_window_filter_present` | Whether at least one query referenced the case investigation window |\n",
                "| `trace_window_violation_count` | How many queries used timestamps outside the case investigation window |\n",
                "| `trace_redundant_query_ratio` | Fraction of queries that were exact duplicates of a previous query in the same run |\n",
                "\n",
                "These metrics catch issues that would be invisible from the final output alone. An agent could produce a perfect verdict by hallucinating rather than querying, issue write queries, look at data outside the permitted window, or waste its context budget on redundant queries.\n",
                "\n",
                "### 2.3 Run-level graders\n",
                "\n",
                "Run-level graders receive all item results after the experiment finishes and compute aggregate classification metrics across the full dataset.\n",
                "\n",
                "| Metric | What it measures |\n",
                "|---|---|\n",
                "| `is_laundering_precision` | Precision for laundering detection across all cases |\n",
                "| `is_laundering_recall` | Recall for laundering detection across all cases |\n",
                "| `is_laundering_f1` | F1 score for laundering detection |\n",
                "| `pattern_type_macro_f1` | Macro-averaged F1 across all pattern types |\n",
                "| `pattern_type_confusion_matrix` | Full confusion matrix over pattern types (stored in metadata) |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Running the Experiment\n",
                "\n",
                "The experiment runs in two passes.\n",
                "\n",
                "Pass 1 executes the task over every dataset item up to `max_concurrency` cases at a time. For each item, the `AmlInvestigationTask` sends the `CaseFile` JSON to the agent, streams its response, and parses it into an `AnalystOutput`. The item-level graders score immediately after each item finishes, while the run-level graders wait until all items are done and then compute aggregate metrics.\n",
                "\n",
                "Pass 2 waits for all traces to be fully ingested by Langfuse, then runs the trace-level graders. This second pass is necessary because trace data arrives asynchronously: the agent may finish producing output before all intermediate tool-call spans have been written to Langfuse.\n",
                "\n",
                "> **Note:** This cell makes live LLM calls for every case in the dataset. With 16 cases and `max_concurrency=4`, expect 10 to 15 minutes total depending on model latency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "task = AmlInvestigationTask()\n",
                "\n",
                "result = run_experiment_with_trace_evals(\n",
                "    DATASET_NAME,\n",
                "    name=\"aml-investigation-baseline\",\n",
                "    task=task,\n",
                "    evaluators=[item_level_deterministic_grader],\n",
                "    trace_evaluators=[trace_deterministic_grader],\n",
                "    run_evaluators=[run_level_grader],\n",
                "    description=\"Baseline AML investigation agent evaluation.\",\n",
                "    max_concurrency=4,\n",
                ")\n",
                "\n",
                "print(\"Experiment complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "await task.close()\n",
                "print(\"Task closed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Inspecting Results\n",
                "\n",
                "The `result` object gives programmatic access to every item-level score and to the trace evaluations. Run-level aggregate metrics are computed by the `run_level_grader` and pushed to the Langfuse experiment run; the most convenient place to read them is the Langfuse UI.\n",
                "\n",
                "### 4.1 Item-level scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rows = []\n",
                "for item_result in result.experiment.item_results:\n",
                "    item = item_result.item\n",
                "    case_input = item.get(\"input\") if isinstance(item, dict) else item.input\n",
                "    expected = item.get(\"expected_output\") if isinstance(item, dict) else item.expected_output\n",
                "\n",
                "    row = {\n",
                "        \"case_id\": case_input.get(\"case_id\", \"\")[:12] + \"...\",\n",
                "        \"trigger_label\": case_input.get(\"trigger_label\", \"\"),\n",
                "        \"gt_laundering\": expected.get(\"is_laundering\"),\n",
                "        \"gt_pattern\": expected.get(\"pattern_type\"),\n",
                "    }\n",
                "\n",
                "    if item_result.output:\n",
                "        output = item_result.output\n",
                "        row[\"pred_laundering\"] = output.get(\"is_laundering\")\n",
                "        row[\"pred_pattern\"] = output.get(\"pattern_type\").value\n",
                "    else:\n",
                "        row[\"pred_laundering\"] = None\n",
                "        row[\"pred_pattern\"] = None\n",
                "\n",
                "    for evaluation in item_result.evaluations or []:\n",
                "        row[evaluation.name] = round(evaluation.value, 3) if evaluation.value is not None else None\n",
                "\n",
                "    rows.append(row)\n",
                "\n",
                "item_df = pd.DataFrame(rows)\n",
                "print(item_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mean score per metric across all cases\n",
                "score_cols = [\n",
                "    \"is_laundering_correct\",\n",
                "    \"pattern_type_correct\",\n",
                "    \"non_laundering_pattern_consistent\",\n",
                "    \"non_laundering_flags_empty\",\n",
                "    \"id_precision_like\",\n",
                "    \"id_coverage\",\n",
                "]\n",
                "available = [c for c in score_cols if c in item_df.columns]\n",
                "item_df[available].mean().rename(\"mean\").to_frame()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Scores by case type\n",
                "\n",
                "Breaking down `is_laundering_correct` by case type shows where the agent struggles. True positives and true negatives are the easiest; false positives and false negatives are where most agents lose points."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "_LOW_SIGNAL = {\"QA_SAMPLE\", \"RANDOM_REVIEW\", \"RETROSPECTIVE_REVIEW\", \"MODEL_MONITORING_SAMPLE\"}\n",
                "_HIGH_SIGNAL = {\"ANOMALOUS_BEHAVIOR_ALERT\", \"LAW_ENFORCEMENT_REFERRAL\", \"EXTERNAL_TIP\"}\n",
                "_PATTERN_LABELS = {\n",
                "    \"FAN-IN\",\n",
                "    \"FAN-OUT\",\n",
                "    \"CYCLE\",\n",
                "    \"GATHER-SCATTER\",\n",
                "    \"SCATTER-GATHER\",\n",
                "    \"STACK\",\n",
                "    \"RANDOM\",\n",
                "    \"BIPARTITE\",\n",
                "}\n",
                "\n",
                "\n",
                "def classify_row(row):\n",
                "    \"\"\"Classify a case based on its trigger label and laundering status.\"\"\"\n",
                "    label = row[\"trigger_label\"]\n",
                "    is_laundering = row[\"gt_laundering\"]\n",
                "    if label in _PATTERN_LABELS and is_laundering:\n",
                "        return \"True Positive\"\n",
                "    if label in _LOW_SIGNAL and not is_laundering:\n",
                "        return \"True Negative\"\n",
                "    if (label in _HIGH_SIGNAL or label in _PATTERN_LABELS) and not is_laundering:\n",
                "        return \"False Positive\"\n",
                "    if label in _LOW_SIGNAL and is_laundering:\n",
                "        return \"False Negative\"\n",
                "    return \"Other\"\n",
                "\n",
                "\n",
                "item_df[\"case_type\"] = item_df.apply(classify_row, axis=1)\n",
                "\n",
                "if \"is_laundering_correct\" in item_df.columns:\n",
                "    breakdown = (\n",
                "        item_df.groupby(\"case_type\")[[\"is_laundering_correct\", \"pattern_type_correct\", \"id_coverage\"]].mean().round(3)\n",
                "    )\n",
                "    print(breakdown.to_string())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Trace-level scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if result.trace_evaluations:\n",
                "    trace_rows = []\n",
                "    for trace_id, evaluations in result.trace_evaluations.evaluations_by_trace_id.items():\n",
                "        row = {\"trace_id\": trace_id[:12] + \"...\"}\n",
                "        for evaluation in evaluations or []:\n",
                "            row[evaluation.name] = round(evaluation.value, 3) if evaluation.value is not None else None\n",
                "        trace_rows.append(row)\n",
                "\n",
                "    trace_df = pd.DataFrame(trace_rows)\n",
                "    print(trace_df.to_string(index=False))\n",
                "else:\n",
                "    print(\"No trace evaluations available.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mean trace scores across all cases\n",
                "trace_score_cols = [\n",
                "    \"trace_has_sql_queries\",\n",
                "    \"trace_read_only_query_check\",\n",
                "    \"trace_window_filter_present\",\n",
                "    \"trace_window_violation_count\",\n",
                "    \"trace_redundant_query_ratio\",\n",
                "]\n",
                "if result.trace_evaluations:\n",
                "    available_trace = [c for c in trace_score_cols if c in trace_df.columns]\n",
                "    print(trace_df[available_trace].mean().rename(\"mean\").to_frame())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.4 Run-level aggregate metrics\n",
                "\n",
                "The run-level grader computes precision, recall, F1 for laundering detection and macro F1 for pattern classification across all cases. These are uploaded to Langfuse and visible in the experiment run summary. You can also extract them from the `ExperimentResult` object returned by the first pass."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"{'Metric':<40} {'Value'}\")\n",
                "print(\"-\" * 50)\n",
                "for evaluation in result.experiment.run_evaluations:\n",
                "    if evaluation.name != \"pattern_type_confusion_matrix\":  # shown separately below\n",
                "        print(f\"{evaluation.name:<40} {evaluation.value:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pattern confusion matrix\n",
                "cm_eval = next((e for e in result.experiment.run_evaluations if e.name == \"pattern_type_confusion_matrix\"), None)\n",
                "if cm_eval and cm_eval.metadata:\n",
                "    labels = cm_eval.metadata.get(\"labels\", [])\n",
                "    matrix = cm_eval.metadata.get(\"matrix\", [])\n",
                "    if labels and matrix:\n",
                "        cm_df = pd.DataFrame(matrix, index=labels, columns=labels)\n",
                "        cm_df.index.name = \"actual \\\\ predicted\"\n",
                "        print(cm_df.to_string())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Exploring Traces in the Langfuse UI\n",
                "\n",
                "The metrics above tell you _what_ the agent got right or wrong, but not _why_. To understand reasoning failures, the Langfuse UI is the right tool. Each experiment item links to the full trace, where you can see every SQL query the agent issued, the model's intermediate reasoning steps, and where in the investigation it went off track.\n",
                "\n",
                "A few things worth looking at in the UI:\n",
                "\n",
                "- **False negative cases** where `is_laundering_correct = 0`: did the agent query the database at all? Did it query the right accounts? Did it stop too early?\n",
                "- **False positive cases** where `is_laundering_correct = 0`: did the agent over-weight the `trigger_label`? Did it find a real pattern or hallucinate one?\n",
                "- **Cases with high `trace_redundant_query_ratio`**: the agent is burning context on repeated queries, which may indicate the prompt strategy for query budgeting is not working.\n",
                "- **Cases with `trace_window_violation_count > 0`**: the agent is looking at transactions outside the permitted window, which would be a compliance issue in a real deployment.\n",
                "\n",
                "## 6. Iterating on the Agent\n",
                "\n",
                "The evaluation pipeline is designed to make iteration straightforward. The dataset in Langfuse is persistent: once uploaded, you do not need to re-upload it. To evaluate a modified agent, call `run_experiment_with_trace_evals` again with a new `name` argument. Langfuse will create a new experiment run and you can compare runs side by side in the UI.\n",
                "\n",
                "Common levers to explore:\n",
                "\n",
                "- **System prompt changes**: edit `ANALYST_PROMPT` in `agent.py` and re-run the experiment. For example, adjust the query strategy section to reduce redundant queries, or revise the typology descriptions to improve pattern classification.\n",
                "- **Temperature and sampling**: pass `temperature=0.0` to `create_aml_investigation_agent` for more deterministic outputs, or increase it to probe sensitivity.\n",
                "- **Lookback window**: change `lookback_days` when generating cases to make investigation windows wider or narrower and see how the agent adapts.\n",
                "- **Case mix**: adjust the ratio of true positives, false positives, and false negatives in `build_cases` to stress-test specific failure modes."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
