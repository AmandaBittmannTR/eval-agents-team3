{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 01: Why Evaluate Agents?\n",
    "\n",
    "LLMs are probabilistic. Agents are LLMs orchestrating tools across multiple steps, and errors can compound easily. A model even if it has 95% chance of being correct on a single step, a 10-step task might only complete successfully ~60% of the time.\n",
    "\n",
    "Evaluation is how we move from \"it seems to work\" to \"we know it works, and we can measure how well.\"\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. The four dimensions of agent quality\n",
    "2. Types of graders used to evaluate agents\n",
    "3. Capability-driven improvement cycles\n",
    "4. How evaluation results drive concrete system changes\n",
    "\n",
    "No code execution required — this is a conceptual foundation for the rest of the bootcamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1",
   "metadata": {},
   "source": [
    "## 1. The Evaluation Framework\n",
    "\n",
    "Agent quality has four dimensions that must be tracked separately — a correct final answer can still involve poor tool usage or incoherent reasoning.\n",
    "\n",
    "- **Outcome Success** — Did the agent achieve the objective?\n",
    "- **Tool Usage Quality** — Right tools, right arguments, right order, no redundancy\n",
    "- **Reasoning Coherence** — Were intermediate steps logical and justified?\n",
    "- **Cost-Performance** — Was the result worth the latency and tokens spent?\n",
    "\n",
    "The pipeline below the pillars shows the evaluation workflow: a **golden dataset** (ground truth) → **hybrid judges** → **go/no-go gate** for production readiness.\n",
    "\n",
    "<img src=\"agent_evaluation_framework.png\" width=\"900\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2",
   "metadata": {},
   "source": [
    "## 2. Types of Graders\n",
    "\n",
    "Not all agent outputs can be judged the same way. We generally know of three grader types that fill complementary roles, and the right mix depends on what you're evaluating.\n",
    "\n",
    "### Code-Based Graders\n",
    "Rule-based checks: string matching (exact, regex, fuzzy), schema validation, tool call verification, outcome assertions, and transcript analysis.\n",
    "\n",
    "- **Strengths:** Fast, cheap, deterministic, easy to debug\n",
    "- **Weaknesses:** Brittle when valid variations exist; no nuance for open-ended outputs\n",
    "\n",
    "### Model-Based Graders (LLM-as-Judge)\n",
    "An LLM evaluates the agent output using rubrics, natural language assertions, pairwise comparison, or reference-based scoring. Multi-judge consensus reduces noise.\n",
    "\n",
    "- **Strengths:** Flexible, scalable, handles subjective and freeform tasks\n",
    "- **Weaknesses:** Non-deterministic, more expensive, requires calibration against human judgments\n",
    "\n",
    "### Human Graders\n",
    "Subject matter experts or crowd annotators review outputs, often via spot-check sampling or A/B testing. Inter-annotator agreement is measured to ensure reliability.\n",
    "\n",
    "- **Strengths:** Gold-standard quality; essential for calibrating model-based graders\n",
    "- **Weaknesses:** Slow and expensive; doesn't scale to continuous deployment\n",
    "\n",
    "### How to Combine Them\n",
    "\n",
    "| Grader Type | Best For | Trade-off |\n",
    "|-------------|----------|-----------|\n",
    "| **Code-based** | Format, schema, tool call checks | Fast but brittle |\n",
    "| **Model-based** | Reasoning quality, nuanced correctness | Flexible but non-deterministic |\n",
    "| **Human** | Calibration, edge cases, expert tasks | Accurate but slow and costly |\n",
    "\n",
    "Scores can be binary, weighted, or hybrid — combining grader types compensates for each approach's individual weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3",
   "metadata": {},
   "source": [
    "## 3. Capability Evaluation: Hills to Climb\n",
    "\n",
    "Hard benchmarks serve a different purpose from correctness checks: they define long-term improvement targets. A 40% pass rate on a difficult benchmark isn't a failure — it's a starting point.\n",
    "\n",
    "The improvement cycle:\n",
    "\n",
    "1. **Test** on hard tasks (low initial pass rate is expected)\n",
    "2. **Identify gaps** — where exactly does the agent fail?\n",
    "3. **Improve** — prompt, tooling, or model changes\n",
    "4. **Re-test** — measure progress; check for regressions\n",
    "\n",
    "SWE-bench Verified is a real example: coding agents went from ~40% to 80%+ pass rate over one year of this cycle.\n",
    "\n",
    "<img src=\"capability_evaluation.png\" width=\"900\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4",
   "metadata": {},
   "source": [
    "## 4. From Evaluation Results to System Changes\n",
    "\n",
    "Evaluation only creates value if failures map to concrete improvements. Each failure pattern has a corresponding intervention:\n",
    "\n",
    "- **Prompt** — rewrite instructions based on failure patterns; reduce token count\n",
    "- **Tool Usage** — fix inefficient sequencing; eliminate redundant calls\n",
    "- **Behavior** — address over-engineering or verbosity identified in transcripts\n",
    "- **Planning & Reasoning** — add verification-aware subgoals; introduce self-critique\n",
    "\n",
    "The loop closes when you re-run the same benchmark and see the delta.\n",
    "\n",
    "<img src=\"system_optimization.png\" width=\"900\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. **Compounding errors** — 10-step agents need task-level evaluation, not just per-answer accuracy\n",
    "2. **Four dimensions** — outcome, tool quality, reasoning, cost-performance\n",
    "3. **Three grader types** — code-based (fast, deterministic), model-based (flexible, nuanced), human (gold-standard, for calibration)\n",
    "4. **Capability benchmarks** — hard problems as hills to climb; progress is measured, not estimated\n",
    "5. **Closed loop** — failure patterns map to prompt, tool, behavior, and reasoning changes\n",
    "\n",
    "**Next:** In Notebook 02, we'll use the shared evaluation harness to run these ideas in code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
