{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 02: The Evaluation Harness\n",
    "\n",
    "The shared evaluation harness (`aieng.agent_evals.evaluation`) provides three composable building blocks:\n",
    "\n",
    "1. **Datasets** — persistent (input, expected_output) collections stored in Langfuse\n",
    "2. **Task functions** — async callables that run your agent and return its output as a string\n",
    "3. **Evaluator functions** — async callables that score output against expected output\n",
    "\n",
    "These compose into `run_experiment`, which handles scheduling, Langfuse integration, and result collection.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Uploading a dataset to Langfuse\n",
    "2. Writing task and evaluator functions\n",
    "3. Using the built-in `create_llm_as_judge_evaluator`\n",
    "4. Running `run_experiment` and reading `ExperimentResult`\n",
    "5. Two-pass evaluation with trace-level graders\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- `GOOGLE_API_KEY` in `.env` — used for the LLM judge via Google's OpenAI-compatible API\n",
    "- `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` in `.env`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yw5p68dv4da",
   "source": "> **Config note:** `aieng-eval-agents/aieng/agent_evals/configs.py` is the single source of global shared configuration (API keys, model names, Langfuse credentials). It contains an OpenAI-compatible client (`openai_api_key` + `openai_base_url`) used **only for LLM-as-a-judge** — this lets you point the judge at any OpenAI-compatible endpoint (Gemini, OpenAI, etc.). Agent implementations, by contrast, use **google-adk** directly, which reads `GOOGLE_API_KEY` independently of the OpenAI client config.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "import contextlib\nimport os\nfrom pathlib import Path\nfrom typing import Any\n\nfrom aieng.agent_evals.async_client_manager import AsyncClientManager\nfrom aieng.agent_evals.evaluation import run_experiment, run_experiment_with_trace_evals\nfrom aieng.agent_evals.evaluation.graders import create_llm_as_judge_evaluator\nfrom aieng.agent_evals.evaluation.graders.config import LLMRequestConfig\nfrom aieng.agent_evals.evaluation.trace import extract_trace_metrics\nfrom aieng.agent_evals.evaluation.types import TraceWaitConfig\nfrom aieng.agent_evals.langfuse import init_tracing\nfrom dotenv import load_dotenv\nfrom IPython.display import HTML, display  # noqa: A004\nfrom langfuse import Langfuse\nfrom langfuse.experiment import Evaluation\nfrom openai import AsyncOpenAI\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\n\n\nif Path(\"\").absolute().name == \"eval-agents\":\n    print(f\"Working directory: {Path('').absolute()}\")\nelse:\n    os.chdir(Path(\"\").absolute().parent.parent)\n    print(f\"Working directory set to: {Path('').absolute()}\")\n\nload_dotenv(verbose=True)\nconsole = Console(width=100)\n\nDATASET_NAME = \"capital-cities-basics\"\ntracing_enabled = init_tracing()"
  },
  {
   "cell_type": "markdown",
   "id": "s1",
   "metadata": {},
   "source": [
    "## 1. Datasets\n",
    "\n",
    "A Langfuse dataset is a persistent collection of evaluation items. Each item has:\n",
    "\n",
    "- **`input`** — passed to the task function (and to evaluators as context)\n",
    "- **`expected_output`** — ground truth used by evaluators; never shown to the agent\n",
    "- **`metadata`** — optional dict for filtering or passing context to evaluators\n",
    "\n",
    "Setting a deterministic `id` on each item makes uploads idempotent — safe to re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = Langfuse()\n",
    "\n",
    "qa_items = [\n",
    "    {\"input\": \"What is the capital of France?\", \"expected_output\": \"Paris\"},\n",
    "    {\"input\": \"What is the capital of Japan?\", \"expected_output\": \"Tokyo\"},\n",
    "    {\"input\": \"What is the capital of Brazil?\", \"expected_output\": \"Brasília\"},\n",
    "    {\"input\": \"What is the capital of Australia?\", \"expected_output\": \"Canberra\"},\n",
    "    {\"input\": \"What is the capital of Canada?\", \"expected_output\": \"Ottawa\"},\n",
    "]\n",
    "\n",
    "with contextlib.suppress(Exception):\n",
    "    langfuse.create_dataset(name=DATASET_NAME)\n",
    "\n",
    "for i, item in enumerate(qa_items):\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=DATASET_NAME,\n",
    "        id=f\"{DATASET_NAME}-{i}\",  # deterministic id → idempotent\n",
    "        input=item[\"input\"],\n",
    "        expected_output=item[\"expected_output\"],\n",
    "    )\n",
    "\n",
    "console.print(f\"[green]✓[/green] Dataset '[cyan]{DATASET_NAME}[/cyan]' ready ({len(qa_items)} items)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2",
   "metadata": {},
   "source": [
    "## 2. Task Functions\n",
    "\n",
    "A task function receives one dataset item and returns the agent's output as a string.\n",
    "The function must accept `item` as a keyword argument; extra arguments go in `**kwargs`.\n",
    "\n",
    "```python\n",
    "async def my_task(*, item: Any, **kwargs: Any) -> str:\n",
    "    ...\n",
    "```\n",
    "\n",
    "`item.input` holds the question. `item.expected_output` is available but must not be used\n",
    "by the task — only by evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task-fn",
   "metadata": {},
   "outputs": [],
   "source": "async def capital_city_task(*, item: Any, **kwargs: Any) -> str:\n    \"\"\"Answer a capital city question using a chat completion model.\"\"\"\n    # Langfuse runs each task in a new thread with its own event loop.\n    # A shared (singleton) async client would be bound to a previous thread's\n    # closed event loop and fail. Use async with to create and close a fresh\n    # client within the current event loop.\n    configs = AsyncClientManager().configs\n    async with AsyncOpenAI(\n        api_key=configs.openai_api_key.get_secret_value(),\n        base_url=configs.openai_base_url,\n    ) as client:\n        response = await client.chat.completions.create(\n            model=configs.default_worker_model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"Answer in one sentence.\"},\n                {\"role\": \"user\", \"content\": item.input},\n            ],\n        )\n    return response.choices[0].message.content or \"\""
  },
  {
   "cell_type": "markdown",
   "id": "s3",
   "metadata": {},
   "source": [
    "## 3. Evaluator Functions\n",
    "\n",
    "An evaluator receives `input`, `output`, and `expected_output` and returns one or more\n",
    "`Evaluation` objects — each a (name, numeric value) pair with an optional comment.\n",
    "\n",
    "### Custom Evaluator\n",
    "\n",
    "The simplest form: a direct assertion on the output string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def substring_match(\n",
    "    *,\n",
    "    output: str,\n",
    "    expected_output: str,\n",
    "    **kwargs: Any,\n",
    ") -> list[Evaluation]:\n",
    "    \"\"\"Score 1.0 if expected_output appears in output (case-insensitive).\"\"\"\n",
    "    correct = expected_output.strip().lower() in output.lower()\n",
    "    return [Evaluation(name=\"substring_match\", value=float(correct))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3b",
   "metadata": {},
   "source": [
    "### Built-in: LLM-as-Judge\n",
    "\n",
    "`create_llm_as_judge_evaluator` returns an evaluator backed by an OpenAI model.\n",
    "Provide a rubric in Markdown; the model scores each criterion 0 or 1 and returns\n",
    "an explanation for each score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = (\n",
    "    \"- **correctness**: 1 if the output names the correct capital city, else 0.\\n\"\n",
    "    \"- **conciseness**: 1 if the answer is one sentence or less, else 0.\\n\"\n",
    ")\n",
    "\n",
    "llm_judge = create_llm_as_judge_evaluator(\n",
    "    name=\"capital_judge\",\n",
    "    rubric_markdown=rubric,\n",
    "    model_config=LLMRequestConfig(temperature=0.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4",
   "metadata": {},
   "source": [
    "## 4. Running `run_experiment`\n",
    "\n",
    "`run_experiment` fetches the dataset from Langfuse, runs the task on every item\n",
    "(with configurable concurrency), applies each evaluator, and records scores back to Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_experiment(\n",
    "    DATASET_NAME,\n",
    "    name=\"capital-cities-v1\",\n",
    "    task=capital_city_task,\n",
    "    evaluators=[substring_match, llm_judge],\n",
    "    description=\"gpt-4o-mini on capital city questions.\",\n",
    "    max_concurrency=5,\n",
    ")\n",
    "\n",
    "console.print(\"[green]✓[/green] Experiment complete\")\n",
    "if result.dataset_run_url:\n",
    "    display(\n",
    "        HTML(f'<p>View results: <a href=\"{result.dataset_run_url}\" target=\"_blank\">{result.dataset_run_url}</a></p>')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table(title=\"Item Results\")\n",
    "table.add_column(\"Question\", style=\"white\", max_width=38)\n",
    "table.add_column(\"Output\", style=\"dim\", max_width=32)\n",
    "table.add_column(\"substring_match\", justify=\"center\", style=\"cyan\")\n",
    "table.add_column(\"correctness\", justify=\"center\", style=\"green\")\n",
    "table.add_column(\"conciseness\", justify=\"center\", style=\"blue\")\n",
    "\n",
    "for item_result in result.item_results:\n",
    "    scores = {e.name: e.value for e in (item_result.evaluations or [])}\n",
    "    q = str(item_result.item.input)\n",
    "    q = q[:36] + \"...\" if len(q) > 36 else q\n",
    "    out = str(item_result.output or \"\")\n",
    "    out = out[:30] + \"...\" if len(out) > 30 else out\n",
    "    table.add_row(\n",
    "        q,\n",
    "        out,\n",
    "        str(scores.get(\"substring_match\", \"-\")),\n",
    "        str(scores.get(\"correctness\", \"-\")),\n",
    "        str(scores.get(\"conciseness\", \"-\")),\n",
    "    )\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5",
   "metadata": {},
   "source": [
    "## 5. Two-Pass Evaluation: Trace Graders\n",
    "\n",
    "Some criteria can only be assessed from the execution trace — for example, latency, tool call counts,\n",
    "or whether the agent's answer is grounded in evidence it actually retrieved.\n",
    "\n",
    "`run_experiment_with_trace_evals` adds a second pass after traces have ingested into Langfuse:\n",
    "\n",
    "1. **Pass 1** — output-based evaluators (same as `run_experiment`)\n",
    "2. **Pass 2** — trace-based evaluators; waits for ingestion, then scores each trace\n",
    "\n",
    "A trace evaluator receives the full `trace` object and the `item_result`:\n",
    "\n",
    "```python\n",
    "def my_trace_eval(*, trace, item_result, **kwargs) -> list[Evaluation]:\n",
    "    ...\n",
    "```\n",
    "\n",
    "`create_trace_groundedness_evaluator` is the built-in grader for tool-using agents —\n",
    "it checks whether the output's claims are supported by tool observations in the trace.\n",
    "For a simple LLM call with no tools, a latency evaluator is used here instead.\n",
    "\n",
    "The `trace_wait` parameter controls how long to wait for Langfuse ingestion before\n",
    "timing out (default: 180 s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "two-pass",
   "metadata": {},
   "outputs": [],
   "source": "def trace_latency_evaluator(*, trace, item_result, **kwargs):\n    \"\"\"Score latency from the trace — works for any traced task.\"\"\"\n    metrics = extract_trace_metrics(trace)\n    if metrics.latency_sec is None:\n        return []\n    return [Evaluation(name=\"latency_sec\", value=metrics.latency_sec)]\n\n\nif tracing_enabled:\n    result_with_traces = run_experiment_with_trace_evals(\n        DATASET_NAME,\n        name=\"capital-cities-v1-traced\",\n        task=capital_city_task,\n        evaluators=[substring_match],\n        trace_evaluators=[trace_latency_evaluator],\n        trace_wait=TraceWaitConfig(max_wait_sec=60),\n        max_concurrency=5,\n    )\n    console.print(\"[green]✓[/green] Two-pass experiment complete\")\n    if result_with_traces.experiment.dataset_run_url:\n        display(\n            HTML(\n                f'<p>View results: <a href=\"{result_with_traces.experiment.dataset_run_url}\"'\n                f' target=\"_blank\">{result_with_traces.experiment.dataset_run_url}</a></p>'\n            )\n        )\nelse:\n    console.print(\n        Panel(\n            \"[dim]Langfuse credentials not found — skipping two-pass evaluation.\\n\"\n            \"Set LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY in .env to enable.[/dim]\",\n            title=\"Note\",\n            border_style=\"dim\",\n        )\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mjwhj3ym49p",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tracing_enabled:\n",
    "    trace_evals = result_with_traces.trace_evaluations\n",
    "    trace_scores = trace_evals.evaluations_by_trace_id  # trace_id → list[Evaluation]\n",
    "\n",
    "    table = Table(title=\"Trace Evaluation Results\")\n",
    "    table.add_column(\"Question\", style=\"white\", max_width=40)\n",
    "    table.add_column(\"latency_sec\", justify=\"right\", style=\"cyan\")\n",
    "    table.add_column(\"Status\", justify=\"center\", style=\"dim\")\n",
    "\n",
    "    for item_result in result_with_traces.experiment.item_results:\n",
    "        q = str(item_result.item.input)\n",
    "        q = q[:38] + \"...\" if len(q) > 38 else q\n",
    "        trace_id = item_result.trace_id\n",
    "\n",
    "        if trace_id and trace_id in trace_scores:\n",
    "            scores = {e.name: e.value for e in trace_scores[trace_id]}\n",
    "            latency = scores.get(\"latency_sec\")\n",
    "            table.add_row(q, f\"{latency:.2f}s\" if latency is not None else \"-\", \"✓\")\n",
    "        elif trace_id in (trace_evals.skipped_trace_ids or []):\n",
    "            table.add_row(q, \"-\", \"skipped\")\n",
    "        elif trace_id in (trace_evals.failed_trace_ids or []):\n",
    "            table.add_row(q, \"-\", \"failed\")\n",
    "        else:\n",
    "            table.add_row(q, \"-\", \"no trace\")\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "    scored = len(trace_scores)\n",
    "    skipped = len(trace_evals.skipped_trace_ids or [])\n",
    "    failed = len(trace_evals.failed_trace_ids or [])\n",
    "    console.print(f\"[dim]Scored: {scored}  Skipped: {skipped}  Failed: {failed}[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | Protocol |\n",
    "|-----------|----------|\n",
    "| **Task function** | `async def task(*, item, **kwargs) -> str` |\n",
    "| **Item evaluator** | `async def eval(*, input, output, expected_output, metadata, **kwargs) -> list[Evaluation]` |\n",
    "| **Trace evaluator** | `async def eval(*, trace, item_result, **kwargs) -> list[Evaluation]` |\n",
    "\n",
    "`run_experiment` composes them: for each dataset item it runs the task and all evaluators,\n",
    "then records scores to Langfuse.\n",
    "\n",
    "Built-in graders:\n",
    "- `create_llm_as_judge_evaluator` — rubric-based output scoring\n",
    "- `create_trace_groundedness_evaluator` — evidence-grounded claim verification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
