{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 01: The DeepSearchQA Dataset & Agent Tools\n",
    "\n",
    "This notebook introduces the two foundational components of the Knowledge QA system:\n",
    "\n",
    "- **DeepSearchQA** — the benchmark dataset used to evaluate the agent\n",
    "- **Agent tools** — the five capabilities the agent uses to research and verify answers\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. What the DeepSearchQA dataset contains and how to explore it\n",
    "2. The five tools the agent has access to, and how it's instructed to use them\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- `GOOGLE_API_KEY` set in your `.env` file\n",
    "- Dependencies installed with `uv sync`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from aieng.agent_evals.knowledge_qa import DeepSearchQADataset\n",
    "from aieng.agent_evals.knowledge_qa.system_instructions import build_system_instructions\n",
    "from dotenv import load_dotenv\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "\n",
    "\n",
    "# Set working directory to the repository root\n",
    "if Path(\"\").absolute().name == \"eval-agents\":\n",
    "    print(f\"Working directory: {Path('').absolute()}\")\n",
    "else:\n",
    "    os.chdir(Path(\"\").absolute().parent.parent)\n",
    "    print(f\"Working directory set to: {Path('').absolute()}\")\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "console = Console(width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-intro",
   "metadata": {},
   "source": [
    "## 1. The DeepSearchQA Dataset\n",
    "\n",
    "[DeepSearchQA](https://www.kaggle.com/datasets/deepmind/deepsearchqa) is a benchmark from Google DeepMind\n",
    "for evaluating deep research agents. It contains 896 research questions requiring multi-step web search\n",
    "and reasoning to answer correctly.\n",
    "\n",
    "Each question is a **causal chain task**: the agent must follow a chain of searches, fetch real sources,\n",
    "and verify facts before answering — not recall from training data.\n",
    "\n",
    "### Answer Types\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Single Answer** | One specific value | A date, a number, a proper name |\n",
    "| **Set Answer** | Multiple required items | A list of countries, a set of policy changes |\n",
    "\n",
    "Evaluation uses an LLM-as-judge that computes **precision, recall, and F1** by comparing the agent's\n",
    "answer to the ground truth item-by-item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DeepSearchQADataset()\n",
    "\n",
    "console.print(f\"Total examples: [cyan]{len(dataset)}[/cyan]\")\n",
    "console.print(f\"Categories: [cyan]{len(dataset.get_categories())}[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-structure",
   "metadata": {},
   "source": [
    "### 1.1 Dataset Structure\n",
    "\n",
    "Each example is a `DSQAExample` with five fields. Let's look at one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]\n",
    "\n",
    "console.print(\n",
    "    Panel(\n",
    "        f\"[bold]example_id:[/bold]       {example.example_id}\\n\"\n",
    "        f\"[bold]problem_category:[/bold] {example.problem_category}\\n\"\n",
    "        f\"[bold]answer_type:[/bold]      {example.answer_type}\\n\\n\"\n",
    "        f\"[bold cyan]problem:[/bold cyan]\\n{example.problem}\\n\\n\"\n",
    "        f\"[bold yellow]answer:[/bold yellow]\\n{example.answer}\",\n",
    "        title=\"DSQAExample\",\n",
    "        border_style=\"blue\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-categories",
   "metadata": {},
   "source": [
    "### 1.2 Categories\n",
    "\n",
    "The dataset spans 17 domains. Let's see how examples are distributed across them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "categories",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset.get_categories()\n",
    "\n",
    "cat_table = Table(title=\"Dataset by Category\")\n",
    "cat_table.add_column(\"Category\", style=\"cyan\")\n",
    "cat_table.add_column(\"Total\", style=\"white\", justify=\"right\")\n",
    "cat_table.add_column(\"Single Answer\", style=\"dim\", justify=\"right\")\n",
    "cat_table.add_column(\"Set Answer\", style=\"dim\", justify=\"right\")\n",
    "\n",
    "for cat in sorted(categories):\n",
    "    examples = dataset.get_by_category(cat)\n",
    "    single = sum(1 for e in examples if e.answer_type == \"Single Answer\")\n",
    "    set_ans = len(examples) - single\n",
    "    cat_table.add_row(cat, str(len(examples)), str(single), str(set_ans))\n",
    "\n",
    "console.print(cat_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-answer-types",
   "metadata": {},
   "source": [
    "### 1.3 Answer Types in Practice\n",
    "\n",
    "The answer type matters for evaluation — the grader treats \"Single Answer\" and \"Set Answer\"\n",
    "differently when computing correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answer-types",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_ex = next(e for e in dataset.examples if e.answer_type == \"Single Answer\")\n",
    "set_ex = next(e for e in dataset.examples if e.answer_type == \"Set Answer\")\n",
    "\n",
    "for label, ex, style in [\n",
    "    (\"Single Answer\", single_ex, \"green\"),\n",
    "    (\"Set Answer\", set_ex, \"yellow\"),\n",
    "]:\n",
    "    console.print(\n",
    "        Panel(\n",
    "            f\"[bold cyan]Question:[/bold cyan]\\n{ex.problem}\\n\\n[bold yellow]Answer:[/bold yellow]\\n{ex.answer}\",\n",
    "            title=f\"{label} — {ex.problem_category}\",\n",
    "            border_style=style,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-browse",
   "metadata": {},
   "source": [
    "### 1.4 Browsing Examples\n",
    "\n",
    "You can retrieve examples by category or by ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples by category\n",
    "finance_examples = dataset.get_by_category(\"Finance & Economics\")\n",
    "console.print(f\"Finance & Economics: [cyan]{len(finance_examples)}[/cyan] examples\\n\")\n",
    "\n",
    "# Display a preview table\n",
    "browse_table = Table(title=\"Finance & Economics — First 5 Examples\")\n",
    "browse_table.add_column(\"ID\", style=\"dim\", width=6)\n",
    "browse_table.add_column(\"Answer Type\", style=\"cyan\", width=15)\n",
    "browse_table.add_column(\"Question\", style=\"white\")\n",
    "\n",
    "for ex in finance_examples[:5]:\n",
    "    q = ex.problem[:75] + \"...\" if len(ex.problem) > 75 else ex.problem\n",
    "    browse_table.add_row(str(ex.example_id), ex.answer_type, q)\n",
    "\n",
    "console.print(browse_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2-intro",
   "metadata": {},
   "source": [
    "## 2. The Agent's Tools\n",
    "\n",
    "The `KnowledgeGroundedAgent` has five tools that form a natural research workflow:\n",
    "\n",
    "| Tool | Purpose | When the Agent Uses It |\n",
    "|------|---------|----------------------|\n",
    "| `google_search` | Find relevant URLs | First step for any sub-question |\n",
    "| `web_fetch` | Read web pages and PDFs | To verify facts from the actual source |\n",
    "| `fetch_file` | Download CSV, XLSX, JSON files | When the answer is in structured data |\n",
    "| `grep_file` | Search within a downloaded file | To locate a specific value in a large file |\n",
    "| `read_file` | Read sections of a downloaded file | To inspect a specific part of a downloaded file |\n",
    "\n",
    "**Why not answer from search snippets?** Snippets are brief and may be outdated or misleading.\n",
    "The system instructions enforce a strict causal chain: **Search → Fetch → Verify → Answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "system-instructions",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = build_system_instructions()\n",
    "\n",
    "console.print(\n",
    "    Panel(\n",
    "        Markdown(instructions),\n",
    "        title=\"Agent System Instructions\",\n",
    "        border_style=\"blue\",\n",
    "        padding=(1, 2),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you saw:\n",
    "\n",
    "1. **The DeepSearchQA dataset** — 896 research questions across 17 categories, evaluated with\n",
    "   precision/recall/F1 using an LLM-as-judge\n",
    "2. **The five agent tools** — search, web fetch, file download, grep, and file read\n",
    "3. **The system instructions** — how the agent is guided to use its tools, including the\n",
    "   critical search → fetch → verify → answer chain\n",
    "\n",
    "**Next:** In Notebook 02, we'll create the agent, run it on questions, and observe how it uses these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "done",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n    Panel(\n        \"[green]✓[/green] Notebook complete!\\n\\n\"\n        \"[cyan]Next:[/cyan] Open [bold]02_running_the_agent.ipynb[/bold] to run the agent.\",\n        title=\"Done\",\n        border_style=\"green\",\n    )\n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
