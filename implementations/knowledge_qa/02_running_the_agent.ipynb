{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 02: Running the Agent\n",
    "\n",
    "In Notebook 01 we explored the dataset and tools. This notebook shows how to run the\n",
    "`KnowledgeGroundedAgent` in practice.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. The agent's PlanReAct architecture and the `AgentResponse` data structure\n",
    "2. Running a question with live progress display\n",
    "3. Inspecting the response: plan, tool calls, sources, and reasoning\n",
    "4. Multi-turn conversations using session state\n",
    "5. Observability with Langfuse tracing\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Complete Notebook 01. You'll need `GOOGLE_API_KEY` in your `.env` file.\n",
    "For tracing (Section 4): `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` are also required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport uuid\nfrom pathlib import Path\n\nfrom aieng.agent_evals.knowledge_qa import KnowledgeGroundedAgent\nfrom aieng.agent_evals.knowledge_qa.notebook import display_response, run_with_display\nfrom aieng.agent_evals.langfuse import init_tracing\nfrom dotenv import load_dotenv\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\n\n\nif Path(\"\").absolute().name == \"eval-agents\":\n    print(f\"Working directory: {Path('').absolute()}\")\nelse:\n    os.chdir(Path(\"\").absolute().parent.parent)\n    print(f\"Working directory set to: {Path('').absolute()}\")\n\nload_dotenv(verbose=True)\nconsole = Console(width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-arch",
   "metadata": {},
   "source": [
    "## 1. Agent Architecture\n",
    "\n",
    "The `KnowledgeGroundedAgent` is built on Google ADK and combines two patterns:\n",
    "\n",
    "**PlanReAct** — Before executing, the agent produces an explicit research plan with numbered\n",
    "steps. Each step has a type (`SEARCH`, `FETCH`, `ANALYZE`) and a status that transitions from\n",
    "`pending` → `in_progress` → `completed` (or `failed`/`skipped`). The plan can be revised\n",
    "mid-run if the agent encounters unexpected results.\n",
    "\n",
    "**ReAct loop** — Within each step, the agent alternates between reasoning (Thought), acting\n",
    "(tool call), and observing (tool response).\n",
    "\n",
    "### The `AgentResponse` Object\n",
    "\n",
    "After running, `agent.answer_async(question)` returns an `AgentResponse`:\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `text` | `str` | The final answer |\n",
    "| `plan` | `ResearchPlan` | Numbered steps with statuses |\n",
    "| `tool_calls` | `list[dict]` | Every tool invocation during execution |\n",
    "| `sources` | `list[GroundingChunk]` | URLs used as evidence |\n",
    "| `reasoning_chain` | `list[str]` | The model's intermediate reasoning |\n",
    "| `total_duration_ms` | `int` | Wall-clock execution time |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = KnowledgeGroundedAgent(enable_planning=True)\n",
    "\n",
    "config_table = Table(title=\"Agent Configuration\", show_header=False)\n",
    "config_table.add_column(\"Setting\", style=\"cyan\")\n",
    "config_table.add_column(\"Value\", style=\"white\")\n",
    "config_table.add_row(\"Model\", agent.model)\n",
    "config_table.add_row(\"Planning\", \"PlanReAct (enabled)\")\n",
    "config_table.add_row(\"Session Service\", \"InMemorySessionService\")\n",
    "config_table.add_row(\"Tools\", \"google_search, web_fetch, fetch_file, grep_file, read_file\")\n",
    "\n",
    "console.print(config_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2-running",
   "metadata": {},
   "source": [
    "## 2. Running a Question\n",
    "\n",
    "`run_with_display` executes the agent in a Jupyter notebook with a live progress display showing:\n",
    "\n",
    "- The research plan with step statuses (updating in real time)\n",
    "- Tool calls as they fire\n",
    "\n",
    "We'll use a question that requires web search — the agent must find and verify a specific fact,\n",
    "not recall it from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When was the highest single-day snowfall recorded in Toronto, and how much snow fell?\"\n",
    "\n",
    "response = await run_with_display(agent, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_response(\n",
    "    console,\n",
    "    response.text,\n",
    "    subtitle=(\n",
    "        f\"Duration: {response.total_duration_ms / 1000:.1f}s  |  \"\n",
    "        f\"Tool calls: {len(response.tool_calls)}  |  \"\n",
    "        f\"Sources: {len(response.sources)}\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2-inspect",
   "metadata": {},
   "source": [
    "### 2.1 Inspecting the Response\n",
    "\n",
    "The `AgentResponse` object contains the full execution trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-plan",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = response.plan\n\nplan_table = Table(title=\"Research Plan\")\nplan_table.add_column(\"#\", style=\"cyan\", width=3)\nplan_table.add_column(\"Step\", style=\"white\")\nplan_table.add_column(\"Type\", style=\"dim\", width=12)\nplan_table.add_column(\"Status\", style=\"green\")\n\nfor step in plan.steps:\n    icon = {\"completed\": \"✓\", \"failed\": \"✗\", \"skipped\": \"○\"}.get(step.status.value, \"·\")\n    desc = step.description[:70] + \"...\" if len(step.description) > 70 else step.description\n    plan_table.add_row(str(step.step_id), desc, step.step_type, f\"{icon} {step.status.value}\")\n\nconsole.print(plan_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.tool_calls:\n",
    "    tools_table = Table(title=\"Tool Calls\")\n",
    "    tools_table.add_column(\"#\", style=\"dim\", width=3)\n",
    "    tools_table.add_column(\"Tool\", style=\"cyan\", width=16)\n",
    "    tools_table.add_column(\"Arguments (truncated)\", style=\"white\")\n",
    "\n",
    "    for i, tc in enumerate(response.tool_calls[:15], 1):\n",
    "        name = tc.get(\"name\", \"unknown\")\n",
    "        args = str(tc.get(\"args\", {}))\n",
    "        args = args[:70] + \"...\" if len(args) > 70 else args\n",
    "        tools_table.add_row(str(i), name, args)\n",
    "\n",
    "    if len(response.tool_calls) > 15:\n",
    "        tools_table.add_row(\"...\", f\"({len(response.tool_calls) - 15} more)\", \"\")\n",
    "\n",
    "    console.print(tools_table)\n",
    "else:\n",
    "    console.print(\"[dim]No tool calls recorded[/dim]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-sources",
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.sources:\n",
    "    seen: set[str] = set()\n",
    "    sources_table = Table(title=\"Sources\")\n",
    "    sources_table.add_column(\"#\", style=\"dim\", width=3)\n",
    "    sources_table.add_column(\"URL\", style=\"blue\")\n",
    "\n",
    "    for src in response.sources:\n",
    "        if src.uri and src.uri not in seen:\n",
    "            seen.add(src.uri)\n",
    "            url = src.uri[:85] + \"...\" if len(src.uri) > 85 else src.uri\n",
    "            sources_table.add_row(str(len(seen)), url)\n",
    "        if len(seen) >= 10:\n",
    "            break\n",
    "\n",
    "    console.print(sources_table)\n",
    "else:\n",
    "    console.print(\"[dim]No sources recorded[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3-multiturn",
   "metadata": {},
   "source": [
    "## 3. Multi-Turn Conversations\n",
    "\n",
    "The agent uses an `InMemorySessionService` to maintain conversation context across turns.\n",
    "Pass the same `session_id` to link questions together — the agent will use prior context\n",
    "when answering follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = str(uuid.uuid4())\n",
    "console.print(f\"Session ID: [dim]{session_id}[/dim]\\n\")\n",
    "\n",
    "# First turn: establish a subject\n",
    "response1 = await agent.answer_async(\n",
    "    \"What is the capital of France?\",\n",
    "    session_id=session_id,\n",
    ")\n",
    "display_response(console, response1.text, title=\"Turn 1\")\n",
    "\n",
    "# Second turn: follow-up that references the prior context\n",
    "response2 = await agent.answer_async(\n",
    "    \"What is the official language spoken there?\",\n",
    "    session_id=session_id,\n",
    ")\n",
    "display_response(console, response2.text, title=\"Turn 2 (follow-up)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4-tracing",
   "metadata": {},
   "source": [
    "## 4. Observability with Langfuse\n",
    "\n",
    "Langfuse captures a full trace of every agent run using OpenTelemetry, giving you visibility into:\n",
    "\n",
    "- Every tool call and its arguments\n",
    "- Every LLM call with prompts and completions\n",
    "- Timing for each span\n",
    "- The full agent execution tree\n",
    "\n",
    "This is essential for debugging failures, measuring latency, and comparing configurations.\n",
    "\n",
    "### Trace Structure\n",
    "\n",
    "```\n",
    "Trace: agent run\n",
    "├── Span: planning (PlanReAct)\n",
    "│   └── LLM Call: create_plan\n",
    "├── Span: step-1-execution\n",
    "│   ├── Tool Call: google_search\n",
    "│   ├── Tool Call: web_fetch\n",
    "│   └── LLM Call: step_summary\n",
    "├── Span: step-2-execution\n",
    "│   └── ...\n",
    "└── Span: synthesis\n",
    "    └── LLM Call: final_answer\n",
    "```\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Set these in your `.env` file:\n",
    "- `LANGFUSE_PUBLIC_KEY`\n",
    "- `LANGFUSE_SECRET_KEY`\n",
    "- `LANGFUSE_HOST` (optional, defaults to `https://cloud.langfuse.com`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-creds",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_configured = all(\n",
    "    [\n",
    "        os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "        os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if langfuse_configured:\n",
    "    console.print(\"[green]✓[/green] Langfuse credentials found\")\n",
    "else:\n",
    "    console.print(\"[yellow]⚠[/yellow] Langfuse credentials not found — tracing cells will be skipped\")\n",
    "    console.print(\"[dim]Set LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY in .env[/dim]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-tracing",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracing_enabled = init_tracing()\n",
    "\n",
    "if tracing_enabled:\n",
    "    console.print(\"[green]✓[/green] Langfuse tracing initialized\")\n",
    "else:\n",
    "    console.print(\"[yellow]⚠[/yellow] Tracing not enabled (check credentials)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-traced",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tracing_enabled:\n",
    "    from langfuse import Langfuse\n",
    "\n",
    "    langfuse = Langfuse()\n",
    "    traced_agent = KnowledgeGroundedAgent(enable_planning=True)\n",
    "    traced_question = \"What programming language was created by Guido van Rossum, and in what year?\"\n",
    "\n",
    "    console.print(Panel(traced_question, title=\"Traced Question\", border_style=\"green\"))\n",
    "\n",
    "    with langfuse.start_as_current_span(name=\"knowledge-agent\", input=traced_question):\n",
    "        trace_id = langfuse.get_current_trace_id()\n",
    "        traced_response = await traced_agent.answer_async(traced_question)\n",
    "        langfuse.update_current_span(output=traced_response.text)\n",
    "\n",
    "    display_response(\n",
    "        console,\n",
    "        traced_response.text,\n",
    "        subtitle=f\"Duration: {traced_response.total_duration_ms / 1000:.1f}s\",\n",
    "    )\n",
    "else:\n",
    "    console.print(\"[dim]Skipping (Langfuse not configured)[/dim]\")\n",
    "    trace_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-traces",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tracing_enabled:\n    from IPython.display import HTML, display  # noqa: A004\n    from langfuse import Langfuse\n    from opentelemetry import trace as otel_trace\n\n    provider = otel_trace.get_tracer_provider()\n    if hasattr(provider, \"force_flush\"):\n        provider.force_flush(timeout_millis=5000)\n    console.print(\"[green]✓[/green] Traces flushed to Langfuse\")\n\n    if trace_id:\n        trace_url = Langfuse().get_trace_url(trace_id=trace_id)\n        display(HTML(f'<p>View trace: <a href=\"{trace_url}\" target=\"_blank\">{trace_url}</a></p>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4-ui",
   "metadata": {},
   "source": [
    "### 4.1 Viewing Traces in the Langfuse UI\n\nOpen your Langfuse project and navigate to **Traces**. Each run appears as a\ntree of spans. Useful things to look at:\n\n- **Span timeline** — which steps take the most time?\n- **Tool call arguments** — what search queries did the agent use?\n- **LLM interactions** — what did the model reason about before calling each tool?\n- **Errors** — red spans show where failures occurred\n\nYou can also filter by trace name, time range, or input content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you learned:\n",
    "\n",
    "1. **Creating the agent** — `KnowledgeGroundedAgent(enable_planning=True)` with PlanReAct\n",
    "2. **Running questions** — `run_with_display` for live notebook progress; `agent.answer_async` for raw access\n",
    "3. **The `AgentResponse`** — plan, tool calls, sources, reasoning, and timing in one object\n",
    "4. **Multi-turn conversations** — linking turns with `session_id`\n",
    "5. **Langfuse tracing** — `init_tracing()` and the Langfuse SDK for full observability\n",
    "\n",
    "**Next:** In Notebook 03, we'll run a systematic evaluation using the DeepSearchQA benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "done",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\n",
    "        \"[green]✓[/green] Notebook complete!\\n\\n\"\n",
    "        \"[cyan]Next:[/cyan] Open [bold]03_evaluation.ipynb[/bold] to evaluate the agent at scale.\",\n",
    "        title=\"Done\",\n",
    "        border_style=\"green\",\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
