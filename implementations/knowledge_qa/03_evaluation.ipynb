{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 03: Evaluation\n",
    "\n",
    "In Notebook 02 we ran individual questions by hand. This notebook evaluates the agent\n",
    "systematically: we upload a dataset subset to Langfuse, run the agent on every item, and\n",
    "score each response with an LLM-as-judge grader using the official DeepSearchQA methodology.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Uploading a DeepSearchQA subset to Langfuse as a persistent dataset\n",
    "2. The LLM-as-judge grader: precision, recall, F1, and the four outcome categories\n",
    "3. A single-sample evaluation walkthrough\n",
    "4. Running the full experiment with `run_experiment`\n",
    "5. Inspecting and interpreting item-level results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Complete Notebooks 01 and 02. You'll need all credentials in `.env`:\n",
    "- `GOOGLE_API_KEY`\n",
    "- `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY`\n",
    "- `OPENAI_API_KEY` (for the LLM grader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from aieng.agent_evals.evaluation import run_experiment\n",
    "from aieng.agent_evals.evaluation.graders.config import LLMRequestConfig\n",
    "from aieng.agent_evals.knowledge_qa import DeepSearchQADataset, KnowledgeGroundedAgent\n",
    "from aieng.agent_evals.knowledge_qa.deepsearchqa_grader import (\n",
    "    EvaluationOutcome,\n",
    "    evaluate_deepsearchqa_async,\n",
    ")\n",
    "from aieng.agent_evals.knowledge_qa.notebook import display_response, run_with_display\n",
    "from aieng.agent_evals.langfuse import upload_dataset_to_langfuse\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import HTML, display  # noqa: A004\n",
    "from langfuse.experiment import Evaluation\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "\n",
    "\n",
    "if Path(\"\").absolute().name == \"eval-agents\":\n",
    "    print(f\"Working directory: {Path('').absolute()}\")\n",
    "else:\n",
    "    os.chdir(Path(\"\").absolute().parent.parent)\n",
    "    print(f\"Working directory set to: {Path('').absolute()}\")\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "console = Console(width=100)\n",
    "\n",
    "DATASET_NAME = \"DeepSearchQA-Subset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-upload-intro",
   "metadata": {},
   "source": [
    "## 1. Uploading the Dataset to Langfuse\n",
    "\n",
    "Langfuse stores our evaluation dataset so we can run multiple experiments against the same items\n",
    "and compare results over time. Each dataset item has three fields:\n",
    "\n",
    "- **`input`**: the question (sent to the agent)\n",
    "- **`expected_output`**: the ground truth answer (given to the grader, never shown to the agent)\n",
    "- **`metadata`**: `category`, `answer_type`, `example_id`\n",
    "\n",
    "Items are deduplicated by a hash of their content, so running this cell again is safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DeepSearchQADataset()\n",
    "examples = dataset.get_by_category(\"Finance & Economics\")[:1]\n",
    "\n",
    "console.print(f\"Uploading [cyan]{len(examples)}[/cyan] examples to dataset '{DATASET_NAME}'...\")\n",
    "\n",
    "# Write examples to a temporary JSONL file for the upload utility\n",
    "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False, encoding=\"utf-8\") as f:\n",
    "    for ex in examples:\n",
    "        record = {\n",
    "            \"input\": ex.problem,\n",
    "            \"expected_output\": ex.answer,\n",
    "            \"metadata\": {\n",
    "                \"example_id\": ex.example_id,\n",
    "                \"category\": ex.problem_category,\n",
    "                \"answer_type\": ex.answer_type,\n",
    "            },\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    temp_path = f.name\n",
    "\n",
    "await upload_dataset_to_langfuse(dataset_path=temp_path, dataset_name=DATASET_NAME)\n",
    "os.unlink(temp_path)\n",
    "\n",
    "console.print(f\"[green]✓[/green] Dataset '{DATASET_NAME}' ready in Langfuse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2-grader-intro",
   "metadata": {},
   "source": [
    "## 2. The DeepSearchQA Grader\n",
    "\n",
    "The grader is an LLM-as-judge that evaluates answers using the official DeepSearchQA methodology\n",
    "from Appendix A of the paper. It handles both answer types:\n",
    "\n",
    "- **Single Answer**: checks whether the response contains the one expected value\n",
    "- **Set Answer**: checks which items from the ground truth set appear in the response,\n",
    "  and flags any extra items the agent included\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Let **S** = predicted items, **G** = ground truth items:\n",
    "\n",
    "| Metric | Formula | Meaning |\n",
    "|--------|---------|---------|\n",
    "| **Precision** | \\|S∩G\\| / \\|S\\| | Of what the agent said, how much was correct |\n",
    "| **Recall** | \\|S∩G\\| / \\|G\\| | Of the ground truth, how much did the agent find |\n",
    "| **F1** | 2·P·R / (P+R) | Harmonic mean of precision and recall |\n",
    "\n",
    "### Outcome Classification\n",
    "\n",
    "| Outcome | Condition | Interpretation |\n",
    "|---------|-----------|----------------|\n",
    "| `fully_correct` | S = G | Perfect answer |\n",
    "| `correct_with_extraneous` | G ⊆ S | All correct, but extra items included |\n",
    "| `partially_correct` | S∩G ≠ ∅ | Some correct items found |\n",
    "| `fully_incorrect` | S∩G = ∅ | No correct items |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2-single-sample",
   "metadata": {},
   "source": [
    "### 2.1 Single-Sample Walkthrough\n",
    "\n",
    "Before running at scale, let's walk through one example end-to-end: run the agent,\n",
    "then grade its response with the LLM judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pick-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibly select one Finance & Economics example\n",
    "finance_examples = dataset.get_by_category(\"Finance & Economics\")\n",
    "example = finance_examples[0]\n",
    "\n",
    "console.print(\n",
    "    Panel(\n",
    "        f\"[bold]ID:[/bold] {example.example_id}\\n\"\n",
    "        f\"[bold]Category:[/bold] {example.problem_category}\\n\"\n",
    "        f\"[bold]Answer Type:[/bold] {example.answer_type}\\n\\n\"\n",
    "        f\"[bold cyan]Question:[/bold cyan]\\n{example.problem}\\n\\n\"\n",
    "        f\"[bold yellow]Ground Truth:[/bold yellow]\\n{example.answer}\",\n",
    "        title=\"Evaluation Example\",\n",
    "        border_style=\"blue\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent = KnowledgeGroundedAgent(enable_planning=True)\n",
    "eval_response = await run_with_display(eval_agent, example.problem)\n",
    "\n",
    "display_response(\n",
    "    console,\n",
    "    eval_response.text,\n",
    "    title=\"Agent Answer\",\n",
    "    subtitle=f\"Duration: {eval_response.total_duration_ms / 1000:.1f}s  |  Tools: {len(eval_response.tool_calls)}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grade-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"[dim]Grading with LLM judge...[/dim]\\n\")\n",
    "\n",
    "result = await evaluate_deepsearchqa_async(\n",
    "    question=example.problem,\n",
    "    answer=eval_response.text,\n",
    "    ground_truth=example.answer,\n",
    "    answer_type=example.answer_type,\n",
    ")\n",
    "\n",
    "outcome_color = {\n",
    "    EvaluationOutcome.FULLY_CORRECT: \"green\",\n",
    "    EvaluationOutcome.CORRECT_WITH_EXTRANEOUS: \"yellow\",\n",
    "    EvaluationOutcome.PARTIALLY_CORRECT: \"orange1\",\n",
    "    EvaluationOutcome.FULLY_INCORRECT: \"red\",\n",
    "}.get(result.outcome, \"white\")\n",
    "\n",
    "metrics_table = Table(title=\"Grader Results\")\n",
    "metrics_table.add_column(\"Metric\", style=\"cyan\")\n",
    "metrics_table.add_column(\"Value\", style=\"white\")\n",
    "metrics_table.add_row(\"Outcome\", f\"[{outcome_color}]{result.outcome.value}[/{outcome_color}]\")\n",
    "metrics_table.add_row(\"Precision\", f\"{result.precision:.3f}\")\n",
    "metrics_table.add_row(\"Recall\", f\"{result.recall:.3f}\")\n",
    "metrics_table.add_row(\"F1\", f\"[bold]{result.f1_score:.3f}[/bold]\")\n",
    "console.print(metrics_table)\n",
    "\n",
    "if result.explanation:\n",
    "    console.print(Panel(result.explanation, title=\"Grader Explanation\", border_style=\"magenta\"))\n",
    "\n",
    "# Show per-item correctness for Set Answer questions\n",
    "if result.correctness_details:\n",
    "    details_table = Table(title=\"Correctness Details\")\n",
    "    details_table.add_column(\"Expected Item\", style=\"white\")\n",
    "    details_table.add_column(\"Found\", style=\"cyan\", justify=\"center\")\n",
    "    for item, found in result.correctness_details.items():\n",
    "        icon = \"[green]✓[/green]\" if found else \"[red]✗[/red]\"\n",
    "        label = item[:60] + \"...\" if len(item) > 60 else item\n",
    "        details_table.add_row(label, icon)\n",
    "    console.print(details_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3-experiment-intro",
   "metadata": {},
   "source": [
    "## 3. Running the Evaluation Experiment\n",
    "\n",
    "`run_experiment` runs the agent against every item in the Langfuse dataset, scores each\n",
    "response, and records results in Langfuse. Each call creates a new named experiment run\n",
    "that you can compare to previous runs in the UI.\n",
    "\n",
    "The experiment takes two functions:\n",
    "\n",
    "- **`agent_task`** — receives a dataset item, runs the agent, returns the answer string\n",
    "- **`deepsearchqa_evaluator`** — receives question, answer, and ground truth; returns grader scores\n",
    "\n",
    "> **Note:** This makes one agent call and one grader call per item. With 10 items and\n",
    "> `max_concurrency=1`, expect 20–40 minutes depending on model latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-task-evaluator",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def agent_task(*, item: Any, **kwargs: Any) -> str:\n",
    "    \"\"\"Run the Knowledge Agent on a Langfuse dataset item.\"\"\"\n",
    "    agent = KnowledgeGroundedAgent(enable_planning=True)\n",
    "    response = await agent.answer_async(item.input)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "async def deepsearchqa_evaluator(\n",
    "    *,\n",
    "    input: str,  # noqa: A002\n",
    "    output: str,\n",
    "    expected_output: str,\n",
    "    metadata: dict[str, Any] | None = None,\n",
    "    **kwargs: Any,\n",
    ") -> list[Evaluation]:\n",
    "    \"\"\"LLM-as-judge grader using DeepSearchQA methodology.\"\"\"\n",
    "    answer_type = (metadata or {}).get(\"answer_type\", \"Set Answer\")\n",
    "    result = await evaluate_deepsearchqa_async(\n",
    "        question=input,\n",
    "        answer=output,\n",
    "        ground_truth=expected_output,\n",
    "        answer_type=answer_type,\n",
    "        model_config=LLMRequestConfig(temperature=0.0),\n",
    "    )\n",
    "    return result.to_evaluations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_result = run_experiment(\n",
    "    DATASET_NAME,\n",
    "    name=\"knowledge-agent-baseline\",\n",
    "    task=agent_task,\n",
    "    evaluators=[deepsearchqa_evaluator],\n",
    "    description=\"Baseline Knowledge Agent on Finance & Economics questions.\",\n",
    "    max_concurrency=1,\n",
    ")\n",
    "\n",
    "console.print(\"[green]✓[/green] Experiment complete\")\n",
    "if experiment_result.dataset_run_url:\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<p>View experiment: <a href=\"{experiment_result.dataset_run_url}\" target=\"_blank\">{experiment_result.dataset_run_url}</a></p>'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4-results-intro",
   "metadata": {},
   "source": [
    "## 4. Inspecting Results\n",
    "\n",
    "The `ExperimentResult` object gives programmatic access to every item-level score.\n",
    "Aggregate metrics are visible in the Langfuse experiment run summary in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "item-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for item_result in experiment_result.item_results:\n",
    "    item = item_result.item\n",
    "    question = str(item.input)\n",
    "    row = {\n",
    "        \"question\": question[:55] + \"...\" if len(question) > 55 else question,\n",
    "        \"answer_type\": (item.metadata or {}).get(\"answer_type\", \"\"),\n",
    "    }\n",
    "    for evaluation in item_result.evaluations or []:\n",
    "        row[evaluation.name] = evaluation.value\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-scores",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of numeric metrics\n",
    "numeric_cols = [c for c in [\"F1\", \"Precision\", \"Recall\"] if c in df.columns]\n",
    "if numeric_cols:\n",
    "    means_table = Table(title=\"Mean Scores\")\n",
    "    means_table.add_column(\"Metric\", style=\"cyan\")\n",
    "    means_table.add_column(\"Mean\", style=\"white\")\n",
    "    for col in numeric_cols:\n",
    "        means_table.add_row(col, f\"{df[col].mean():.3f}\")\n",
    "    console.print(means_table)\n",
    "\n",
    "# Outcome distribution\n",
    "if \"Outcome\" in df.columns:\n",
    "    outcome_table = Table(title=\"Outcome Distribution\")\n",
    "    outcome_table.add_column(\"Outcome\", style=\"cyan\")\n",
    "    outcome_table.add_column(\"Count\", style=\"white\", justify=\"right\")\n",
    "    outcome_table.add_column(\"Fraction\", style=\"dim\", justify=\"right\")\n",
    "    total = len(df)\n",
    "    for outcome, count in df[\"Outcome\"].value_counts().items():\n",
    "        outcome_table.add_row(str(outcome), str(count), f\"{count / total:.0%}\")\n",
    "    console.print(outcome_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5-iteration",
   "metadata": {},
   "source": [
    "## 5. Iterating on the Agent\n",
    "\n",
    "The dataset in Langfuse is persistent — you don't need to re-upload it. To evaluate a modified\n",
    "agent, call `run_experiment` again with a new `name` argument. Langfuse will create a new\n",
    "experiment run and you can compare runs side-by-side in the UI.\n",
    "\n",
    "### Levers to Explore\n",
    "\n",
    "- **System prompt** — edit `SYSTEM_INSTRUCTIONS_TEMPLATE` in `system_instructions.py` to change\n",
    "  the search strategy, verification rules, or final answer format\n",
    "- **Planning** — toggle `enable_planning=False` to skip PlanReAct and compare quality vs. speed\n",
    "- **Model** — change the Gemini model in `KnowledgeGroundedAgent` for different capability/cost trade-offs\n",
    "- **Dataset** — change the `category` filter in Section 1 or increase `samples` to cover more examples\n",
    "\n",
    "### What to Look for in Langfuse\n",
    "\n",
    "- Items with **low F1** — did the agent fail to fetch the source? Stop early? Misread the question?\n",
    "- Items with **`correct_with_extraneous`** — is the agent over-generating? Can the prompt be tightened?\n",
    "- **Latency outliers** — which steps are slow? Is replanning happening unnecessarily?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you:\n",
    "\n",
    "1. **Uploaded** a DeepSearchQA subset to Langfuse as a persistent, reusable dataset\n",
    "2. **Understood** the LLM-as-judge grader: precision, recall, F1, and the four outcome categories\n",
    "3. **Walked through** a single-sample evaluation end-to-end\n",
    "4. **Ran** a full experiment with `run_experiment` and inspected item-level scores\n",
    "5. **Learned** how to iterate: re-run with a new experiment name to compare configurations in Langfuse\n",
    "\n",
    "The evaluation pipeline is the foundation for systematic agent improvement — each iteration\n",
    "produces a new experiment run that you can compare to the baseline in the Langfuse UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "done",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[green]✓[/green] Notebook complete!\", title=\"Done\", border_style=\"green\"))\n",
    "if experiment_result.dataset_run_url:\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<p>View experiment results: <a href=\"{experiment_result.dataset_run_url}\" target=\"_blank\">{experiment_result.dataset_run_url}</a></p>'\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
