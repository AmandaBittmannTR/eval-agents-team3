{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ea5037-7ad4-4e2e-b55d-f879fa9dc436",
   "metadata": {},
   "source": [
    "# Running the Report Generation Agent\n",
    "\n",
    "This notebook runs the **Report Generation Agent** for **single-table relational\n",
    "data source** as a Gradio Demo UI and evaluations with [Langfuse](https://langfuse.com/).\n",
    "\n",
    "The Report Generation Agent Gradio Demo will provide an UI to read **user queries in natural language**\n",
    "and proceed to **make SQL queries** to the database in order to produce the data for\n",
    "the report. At the end, the Agent will provide a **downloadable link** to the report as\n",
    "an `.xlsx` file.\n",
    "\n",
    "This example also provides agent monitoring and evaluations using Langfuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72ee27-a68a-4349-af4f-fde4bca8663a",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "The code below sets the notebook default folder, sets the default constants and checks the presence of the environment variables.\n",
    "\n",
    "The environment variables can be set in the `.env` file in the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b46b94-8e30-4627-9265-4fe09af2a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from aieng.agent_evals.async_client_manager import AsyncClientManager\n",
    "\n",
    "\n",
    "# Setting the notebook directory to the project's root folder\n",
    "if Path(\"\").absolute().name == \"eval-agents\":\n",
    "    print(f\"Notebook path is already the root path: {Path('').absolute()}\")\n",
    "else:\n",
    "    os.chdir(Path(\"\").absolute().parent.parent)\n",
    "    print(f\"The notebook path has been set to: {Path('').absolute()}\")\n",
    "\n",
    "client_manager = AsyncClientManager.get_instance()\n",
    "assert client_manager.configs.report_generation_db.database, (\n",
    "    \"[ERROR] The database path is not set! Please configure the REPORT_GENERATION_DB__DATABASE environment variable.\"\n",
    ")\n",
    "assert client_manager.configs.langfuse_secret_key, (\n",
    "    \"[ERROR] The Langfuse secret key is not set! Please configure the LANGFUSE_SECRET_KEY environment variable.\"\n",
    ")\n",
    "assert client_manager.configs.langfuse_public_key, (\n",
    "    \"[ERROR] The Langfuse public key is not set! Please configure the LANGFUSE_PUBLIC_KEY environment variable.\"\n",
    ")\n",
    "assert client_manager.configs.langfuse_host, (\n",
    "    \"[ERROR] The Langfuse base URL is not set! Please configure the LANGFUSE_BASE_URL environment variable.\"\n",
    ")\n",
    "\n",
    "from implementations.report_generation.demo import start_gradio_app  # noqa: E402\n",
    "\n",
    "\n",
    "print(\"All environment variables have been set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0522bc-01f5-4554-bc57-39de8dbb4c53",
   "metadata": {},
   "source": [
    "## Running the Demo UI\n",
    "\n",
    "The code below will start the **Gradio Demo UI**, which will run embedded in this notebook (made possible by the `enable_public_link=True` parameter). If you so choose, you can also run the UI in your browser using the links provided in the outputs.\n",
    "\n",
    "The UI will display a few **pre-defined input options** to test the agent and also a text box so you can **make your own queries**. The Agent will try to complete the request by making **queries against the database** configured in the first notebook. At the end, it will **produce a report** that it will make available as a download link at the end of the agent's execution.\n",
    "\n",
    "The UI will display the **agents thoughts** and **tool calls** in order to facilitate debugging.\n",
    "\n",
    "The agent **traces will be sent** to [Langfuse](https://us.cloud.langfuse.com/). To configure the langfuse connection, you can edit the following environment variables in your `.env` file:\n",
    "\n",
    "```python\n",
    "# Secret and public keys can be generated by the Langfuse web UI\n",
    "LANGFUSE_SECRET_KEY=\"sk-lf-...\"\n",
    "LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\n",
    "LANGFUSE_BASE_URL=\"https://us.cloud.langfuse.com\"\n",
    "\n",
    "# Input the name of your langfuse project on the first variable\n",
    "# The second variable is used to configure where the output reports are going to be saved\n",
    "# Defaults are in implementations/report_generation/env_vars.py)\n",
    "REPORT_GENERATION_LANGFUSE_PROJECT_NAME=\"...\"\n",
    "REPORT_GENERATION_OUTPUT_PATH=\"...\"\n",
    "```\n",
    "\n",
    "Run the cell below to see the Report Generation Agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceeac6a-309d-484f-98dc-227f875387a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "await start_gradio_app(\n",
    "    enable_trace=True,\n",
    "    enable_public_link=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdee1c5-f5d9-485a-9a48-f5aab2c41d04",
   "metadata": {},
   "source": [
    "## Agent Configuration\n",
    "\n",
    "This agent is a **text-to-SQL** agent that will convert natural language into one or multiple queries to a DB in order to produce the result that has been requested.\n",
    "\n",
    "This agent in particular has **no knowledge of the DB schema**, although it is possible to do so with some more advanced techniques. This allows the agent to be **flexible** to any database it may encounter. However, it is **easier** to have it working with a **single-table DB** as opposed to a multi-table DB.\n",
    "\n",
    "It is configured with **three tools** (as per `aieng.agent_evals.evaluation.report_generation.agent.get_report_generation_agent`):\n",
    "- `db_manager.report_generation_db().get_schema_info`: to retrieve the **DB schema** so the agent knows how to perform the queries.\n",
    "- `db_manager.report_generation_db().execute`: to execute any **read-only SQL queries**.\n",
    "- `report_file_writer.write_xlsx`: a function that receives the **report data** as an array, writes the array to a `.xlsx` file and returns a **downloadable Gradio link** to the file.\n",
    "\n",
    "The Agent will know how to use those tools given the **instructions** it has been given, along with the user input. Below are the instructions for this agent in specific (as per `aieng.agent_evals.evaluation.report_generation.prompts`:\n",
    "```python\n",
    "MAIN_AGENT_INSTRUCTIONS = \"\"\"\\\n",
    "Perform the task using the SQLite database tool. \\\n",
    "EACH TIME before invoking the function, you must explain your reasons for doing so. \\\n",
    "If the SQL query did not return intended results, try again. \\\n",
    "For best performance, divide complex queries into simpler sub-queries. \\\n",
    "Do not make up information. \\\n",
    "When the report is done, use the report file writer tool to write it to a file. \\\n",
    "Make sure the \"write_xlsx\" tool is called so it generates the report file. \\\n",
    "At the end, provide the report file as a downloadable hyperlink to the user. \\\n",
    "Make sure the link can be clicked on by the user.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0398e-944a-4005-a9e3-44dbb050441a",
   "metadata": {},
   "source": [
    "## Online Evaluations\n",
    "\n",
    "The Agent will be sending online evaluation metrics to Langfuse along with the traces. These metrics aim to **simulate** how an evaluation of a **production agentic system** would work for this use case.\n",
    "\n",
    "Those metrics are:\n",
    "- A check of the **total token spent** to complete the request. If it is below the threshold of 15k tokens it will send a score of 1, or a score of 0 otherwise.\n",
    "- A check of the **total time spent** to complete the request. If it is below the threshold of 60 seconds it will send a score of 1, or a score of 0 otherwise.\n",
    "- At the end of the Agent run, the UI will display thumbs up/thumbs down buttons so the **users can send feedback** on the output of the agent.\n",
    "\n",
    "The calculation for the first two metrics are triggered by a **callback function** that runs at the end of the agent run, while the third one is triggered asynchronously by an **user UI action**.\n",
    "\n",
    "Those metrics will be available in Langfuse in two different ways:\n",
    "- **Aggregated:** Summary of those metrics will be displayed on the dashboards page\n",
    "- **Individualized:** The evaluation scores are available on each trace with additional details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
