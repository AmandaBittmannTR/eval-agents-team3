{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ddcbff-a950-42ee-be8b-a3663ae6b202",
   "metadata": {},
   "source": [
    "## Running the Offline Evaluations for the Report Generation Agent\n",
    "\n",
    "Offline evaluations are evaluations run against a **pre-defined dataset**. It performs **detailed evaluations** of the **outputs** of the agentic system and the **steps** it has taken to produce those evaluations.\n",
    "\n",
    "This dataset is called the **expected results** or the **ground-truth** dataset, and on this case it's a **handcrafted** dataset with **inputs, outputs and trajectory** for a few known use cases.\n",
    "\n",
    "The evaluations are run by Langfuse and the results are visualized there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74c461-bcda-49b2-bd4e-5ea961798d55",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "The code below sets the notebook default folder, sets the default constants and checks the presence of the environment variables.\n",
    "\n",
    "The environment variables can be set in the `.env` file in the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfd868-c4da-4bdb-ae13-0ebe56c3ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from aieng.agent_evals.async_client_manager import AsyncClientManager\n",
    "from aieng.agent_evals.langfuse import upload_dataset_to_langfuse\n",
    "\n",
    "\n",
    "# Setting the notebook directory to the project's root folder\n",
    "if Path(\"\").absolute().name == \"eval-agents\":\n",
    "    print(f\"Notebook path is already the root path: {Path('').absolute()}\")\n",
    "else:\n",
    "    os.chdir(Path(\"\").absolute().parent.parent)\n",
    "    print(f\"The notebook path has been set to: {Path('').absolute()}\")\n",
    "\n",
    "client_manager = AsyncClientManager.get_instance()\n",
    "assert client_manager.configs.report_generation_db.database, (\n",
    "    \"[ERROR] The database path is not set! Please configure the REPORT_GENERATION_DB__DATABASE environment variable.\"\n",
    ")\n",
    "assert client_manager.configs.langfuse_secret_key, (\n",
    "    \"[ERROR] The Langfuse secret key is not set! Please configure the LANGFUSE_SECRET_KEY environment variable.\"\n",
    ")\n",
    "assert client_manager.configs.langfuse_public_key, (\n",
    "    \"[ERROR] The Langfuse public key is not set! Please configure the LANGFUSE_PUBLIC_KEY environment variable.\"\n",
    ")\n",
    "assert client_manager.configs.langfuse_host, (\n",
    "    \"[ERROR] The Langfuse base URL is not set! Please configure the LANGFUSE_HOST environment variable.\"\n",
    ")\n",
    "\n",
    "print(\"All environment variables have been set.\")\n",
    "\n",
    "\n",
    "EVALUATION_DATASET_PATH = \"implementations/report_generation/data/OnlineRetailReportEval.json\"\n",
    "LANGFUSE_DATASET_NAME = \"OnlineRetailReportEval\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fe764-9355-40c6-8632-fccca34acce9",
   "metadata": {},
   "source": [
    "## Taking a Look at the Ground Truth Dataset\n",
    "\n",
    "The ground-truth dataset is located at `implementations/report_generation/data/OnlineRetailReportEval.json`. The code below will display one of its elements as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e943f83-7178-43bc-be17-0e550564002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"implementations/report_generation/data/OnlineRetailReportEval.json\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "print(f\"Ground-truth dataset size: {len(ground_truth)}\")\n",
    "print(\"First element:\")\n",
    "pprint(ground_truth[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eeee29-b23e-4bad-b20f-ef32610a5570",
   "metadata": {},
   "source": [
    "Here is an explanation of the data structure of the dataset samples:\n",
    "```python\n",
    "{\n",
    "    'id': str,  # The ID of the sample\n",
    "    'input': str,  # The input to be used to test the report generation agent\n",
    "    'expected_output': {  # The expected outputs of the agent\n",
    "        'final_report': {  # The output data for the final report the agent generates. \n",
    "                           # These values match the input the agent sends to the `write_xlsx` function\n",
    "            'filename': str,  # The name of the report file\n",
    "            'report_columns': list[str,  # The names of the columns of the report\n",
    "            'report_data': list[list[Any]],  # a bidimensional array of values for the rows of the report\n",
    "        }\n",
    "        'trajectory': {  # information about the trajectory the agent should take to produce the report\n",
    "            'actions': list[str],  # A list of the names of the actions the agent should take, in order\n",
    "            'description': list[str],  # A description of what the parameters that are sent to each one of\n",
    "                                       # the actions are supposed to be doing\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e993b7-a93d-468e-84e9-0e1906108335",
   "metadata": {},
   "source": [
    "## Uploading the dataset to Langfuse\n",
    "\n",
    "Use the function below to **upload** the ground truth dataset to Langfuse so it can be used **during the evaluation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd692600-d936-4c7b-af94-5ddd19b59bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "await upload_dataset_to_langfuse(\n",
    "    EVALUATION_DATASET_PATH,\n",
    "    LANGFUSE_DATASET_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc8c175-6315-41bf-800a-d913e3e2b5c0",
   "metadata": {},
   "source": [
    "## LLM-as-a-judge Evaluators\n",
    "\n",
    "Two **LLM-as-a-judge evaluators** are set to run against this dataset and the agent's output:\n",
    "1. A **Final Result Evaluator**, that will evaluate the agent's output against the contents of the `final_result` key\n",
    "2. A **Trajectory Evaluator**, that will evaluate the agent's output against the contents of the `trajectory` key\n",
    "\n",
    "Here are the instructions for both of those agents (as per `aieng.agent_evals.evaluation.report_generation.prompts`):\n",
    "```python\n",
    "TRAJECTORY_EVALUATOR_INSTRUCTIONS = \"\"\"\\\n",
    "You are evaluating if an agent has followed the correct trajectory to generate a report.\\\n",
    "The agent is a Report Generation Agent that uses the SQLite database tool to generate a report\\\n",
    "and return the report as a downloadable file to the user.\\\n",
    "You will be presented with the \"Question\" that has been asked to the agent along with two sets of data:\\\n",
    "- The \"Expected Trajectory\" of the agent, which contains:\\\n",
    "    - A list ids for the actions the agent is expected to perform\\\n",
    "    - A list of rough descriptions of what has been passed as parameters to the actions\\\n",
    "- The \"Actual Trajectory\" of the agent, which contains:\\\n",
    "    - A list ids for the actions the agent performed\\\n",
    "    - A list of parameters that has been passed to each one of the actions\\\n",
    "It's OK if the agent makes mistakes and performs additional steps, or if the queries do not exactly match\\\n",
    "the description, as long as the queries performed end up satisfying the \"Question\".\\\n",
    "It is important that the last action to be of type \"final_response\" and that it produces a link to the report file.\n",
    "\"\"\"\n",
    "\n",
    "RESULT_EVALUATOR_INSTRUCTIONS = \"\"\"\\\n",
    "Evaluate whether the \"Proposed Answer\" to the given \"Question\" matches the \"Ground Truth\". \\\n",
    "Disregard the following aspects when comparing the \"Proposed Answer\" to the \"Ground Truth\": \\\n",
    "- The order of the items should not matter, unless explicitly specified in the \"Question\". \\\n",
    "- The formatting of the values should not matter, unless explicitly specified in the \"Question\". \\\n",
    "- The column and row names have to be similar but not necessarily exact, unless explicitly specified in the \"Question\". \\\n",
    "- The filename has to be similar by name but not necessarily exact, unless explicitly specified in the \"Question\". \\\n",
    "- It is ok if the filename is missing. \\\n",
    "- The numerical values should be equal with a tolerance of 0.01. \\\n",
    "- The report data in the \"Proposed Answer\" should have the same number of rows as in the \"Ground Truth\". \\\n",
    "- It is OK if the report data in the \"Proposed Answer\" contains extra columns or if the rows are in a different order, \\\n",
    "unless explicitly specified in the \"Question\".\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b05537-2624-4df7-9f08-59d797378175",
   "metadata": {},
   "source": [
    "## Running the Evaluations\n",
    "\n",
    "To run those two evaluators against all of the ground-truth dataset samples, run the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d38a16-526a-4740-953a-dc735a9f83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running as a CLI command to avoid issues between Langfuse's\n",
    "# experiment runner and Jupyter\n",
    "# NOTE: This will take a while to execute in a notebook environment\n",
    "# It runs faster when executed in a regular console session\n",
    "\n",
    "!uv run --env-file .env python -m implementations.report_generation.evaluate --max-concurrency 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969ed04-7427-40e9-b51b-fa892da706ef",
   "metadata": {},
   "source": [
    "## Checking the Results\n",
    "\n",
    "At the end of the run, you will see a summary in the console.\n",
    "\n",
    "To see detailed results of the evaluation runs:\n",
    "1. Go to your project on Langfuse\n",
    "2. Click on **Datasets**\n",
    "3. Click on the dataset name\n",
    "4. Click on one of the runs\n",
    "\n",
    "You will see a more detailed summary of the experiment run and also you can see the details of each of of the runs, including f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855cf8f-f386-449a-a777-4a5adee5d2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
